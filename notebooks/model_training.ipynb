{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Distillation\n",
    "\n",
    "In this notebook, we will fine-tune a smaller language model, Falcon-7B Instruct, by following a distillation process using synthetic data generated from a larger model. The purpose of distillation is to transfer the knowledge of the larger model to the smaller one, making it more efficient while retaining high-quality performance for a specific task: responding as a vegan nutritionist.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Generating Synthetic Data:** The larger model will act as a \"teacher\" by generating responses for a given set of prompts, simulating the role of a vegan nutritionist. These responses will form the synthetic dataset that the smaller model will learn from, covering a range of nutritional topics related to vegan diets.\n",
    "\n",
    "- **Fine-Tuning the Smaller Model:** With the synthetic dataset prepared, we will fine-tune the smaller Falcon-7B Instruct model using the teacher model’s outputs as the target responses. This process enables the smaller model to learn from the larger model’s behavior, making it capable of producing relevant responses efficiently.\n",
    "\n",
    "- **Experiment Tracking:** Throughout the fine-tuning process, an experiment tracking system will be used to log hyperparameters, model performance, and training metrics. This will help in tracking progress and identifying the optimal model configuration.\n",
    "\n",
    "- **Next Steps:** Once the fine-tuning process is complete, we will proceed with building a separate inference pipeline and web application to handle real-time interactions. These components will be detailed in future steps of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Data\n",
    "\n",
    "We need to do few shot learning to genearate synthetic data (at least 100 samples). So we will load in some pdf files for example papers and manually create Q&A pairs from them.\n",
    "\n",
    "Steps are as follows:\n",
    "\n",
    "- Load pdf files from s3\n",
    "- Create user_context and question pairs from pdf files (3 examples)\n",
    "- Use OpenAI to generate synthetic data (at least 100 samples) using these examples for few-shot learning\n",
    "- Then we will again use OpenAI to generate outputs for the generated data, but we will also use Rag to get relevant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# move to root directory\n",
    "os.chdir('..')\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Raw PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = os.environ.get('AWS_BUCKET_NAME')\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "response[\"ResponseMetadata\"][\"HTTPStatusCode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'vegan_or_plant_based_nutrition_data.json'\n",
    "\n",
    "# Get the file contents from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Generated Examples\n",
    "\n",
    "Let's take a look at a bunch of abstracts and decide where to pull examples from based on interesting examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following abstracts seem to be good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**10. Abstract**\n",
       "\n",
       "Rheumatoid arthritis is a debilitating inflammatory condition which has a high disease burden. While there is emerging evidence that certain foods and diets could have anti-inflammatory properties and there are published ‘anti-inflammatory’ diets, there is very little understanding of patient beliefs and perceptions about the impact of diet on symptom management or attitudes to particular dietary interventions. This scoping review aims to summarize the existing literature around the beliefs that patients with rheumatoid arthritis hold regarding the impact of diet on disease activity and joint pain. It also examines the current state of evidence regarding the impact of specific dietary interventions on patient reported and objective parameters of RA disease activity. A search was conducted across seven databases for studies which included reporting on dietary beliefs related to disease management or investigations on the effect of particular diets on disease activity or joint pain. Articles were excluded if they examined extracted compounds or individual dietary supplements. Included studies were synthesized narratively. We retrieved 25,585 papers from which 68 were included in this review: 7 assessed dietary beliefs, 61 explored dietary interventions. The available literature on patient beliefs has been largely limited to quantitative studies with limited qualitative exploration. The Mediterranean, fasting and vegan diets appear to have the most benefit with regards to rheumatoid arthritis outcomes for patients. Research which examines RA patient’s beliefs and attitudes about the impact of diet on their RA symptoms and disease is currently lacking."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: What do we know about dietary perceptions and beliefs of patients with rheumatoid arthritis? A scoping review\n",
      "URL: http://dx.doi.org/10.1007/s00296-024-05691-5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**11. Abstract**\n",
       "\n",
       "The present study aimed to produce frozen dessert containing plant-based milk (almond, hazelnut, and lupine) and the probiotic Lb. acidophilus bacteria and to evaluate the chemical, microbiological and sensory properties during the 90 day-storage. Frozen dessert antioxidant capacity at day 0 and 90 of evaluation and changes in the phenolic compounds based on variations between different species were significant ( p  < 0.05). The differences in Lb. acidophilus counts between storage days were significant and values ranged from 4.15–8.99 log CFU/mL on the first day of storage to 3.61–7.06 at the end of the storage. Regarding the results of general acceptability in sensory evaluation, the highest color, taste and aroma scores was determined on day 0 in the hazelnut-lupine milk frozen dessert sample whereas the lowest was determined on day 30 in the almond-lupine milk frozen dessert sample. The samples with the highest antioxidant capacity were found on day 90 day in lupine frozen dessert (87.28 ± 0.007 mM) whereas the samples with the lowest antioxidant capacity were found on day 0 in the almond-hazelnut-lupine frozen dessert (18.83 ± 4.56 mM). Plant-based milk is considered suitable for the main ingredients in ice cream production, due to its health benefits its potential to be consumed as frozen dessert."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Manufacturing plant-based non-dairy and probiotic frozen desserts and their impact on physicochemical, sensory and functional aspects\n",
      "URL: http://dx.doi.org/10.1007/s13197-024-05964-8\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**12. Abstract**\n",
       "\n",
       "Fibromyalgia (FM) is a complex and common syndrome characterized by chronic widespread pain, fatigue, sleep disturbances, and various functional symptoms without clear structural or pathological causes. Affecting approximately 1–5% of the global population, with a higher prevalence in women, FM significantly impacts patients’ quality of life, often leading to considerable healthcare costs and loss of productivity. Despite its prevalence, the etiology of FM remains elusive, with genetic, environmental, and psychological factors, including nutrition, being implicated. Currently, no universally accepted treatment guidelines exist, and management strategies are often symptomatic. This narrative review explores the potential of a neuronutritional approach to FM management. It synthesizes existing research on the relationship between FM and nutrition, suggesting that dietary interventions could be a promising complementary treatment strategy. Various nutritional interventions, including vitamin D, magnesium, iron, and probiotics supplementation, have shown potential in reducing FM symptoms, such as chronic pain, anxiety, depression, cognitive dysfunction, sleep disturbances, and gastrointestinal issues. Additionally, weight loss has been associated with reduced inflammation and improved quality of life in FM patients. The review highlights the anti-inflammatory benefits of plant-based diets and the low-FODMAPs diet, which have shown promise in managing FM symptoms and related gastrointestinal disorders. Supplements such as vitamin D, magnesium, vitamin B12, coenzyme Q10, probiotics, omega-3 fatty acids, melatonin, S-adenosylmethionine, and acetyl- l -carnitine are discussed for their potential benefits in FM management through various mechanisms, including anti-inflammatory effects, modulation of neurotransmitters, and improvement of mitochondrial function. In conclusion, this review underscores the importance of considering neuronutrition as a holistic approach to FM treatment, advocating for further research and clinical trials to establish comprehensive dietary guidelines and to optimize management strategies for FM patients."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Neuronutritional Approach to Fibromyalgia Management: A Narrative Review\n",
      "URL: http://dx.doi.org/10.1007/s40122-024-00641-2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**13. Abstract**\n",
       "\n",
       "Purpose of Review International guidelines emphasize advice to incorporate dietary measures for the prevention and in the management of hypertension. Current data show that modest reductions in weight can have an impact on blood pressure. Reducing salt and marine oils have also shown consistent benefit in reducing blood pressure. Whether other dietary constituents, in particular the amount and type of fat that play important roles in cardiovascular prevention, influence blood pressure sufficiently to be included in the management of hypertension is less certain. In this review, we provide a summary of the most recent findings, with a focus on dietary patterns, fats and other nutrients and their impact on blood pressure and hypertension. Recent Findings Since reducing salt consumption is an established recommendation only corollary dietary advice is subject to the current review. Population studies that have included reliable evaluation of fat intake have indicated almost consistently blood pressure lowering with consumption of marine oils and fats. Results with vegetable oils are inconclusive. However dietary patterns that included total fat reduction and changes in the nature of vegetable fats/oils have suggested beneficial effects on blood pressure. Plant-based foods, dairy foods and yoghurt particularly, may also lower blood pressure irrespective of fat content. Summary Total fat consumption is not directly associated with blood pressure except when it is part of a weight loss diet. Consumption of marine oils has mostly shown moderate blood pressure lowering and possibly greatest effect with docosahexaenoic acid-rich oil."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Diet to Stop Hypertension: Should Fats be Included?\n",
      "URL: http://dx.doi.org/10.1007/s11906-024-01310-7\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**14. Abstract**\n",
       "\n",
       "Fossil fuel-based products should be replaced by products derived from modern biomass such as plant starch, in the context of the future circular economy. Starch production globally surpasses 50 million tons annually, predominantly sourced from maize, rice, and potatoes. Here, we review plant starch with an emphasis on structure and properties, extraction, modification, and green applications. Modification techniques comprise physical, enzymatic, and genetic methods. Applications include stabilization of food, replacement of meat, three-dimensional food printing, prebiotics, encapsulation, bioplastics, edible films, textiles, and wood adhesives. Starch from maize, potatoes, and cassava shows amylose content ranging from 20 to 30% in regular varieties to 70% in high-amylose varieties. Extraction by traditional wet milling achieves starch purity up to 99.5%, while enzymatic methods maintain higher structural integrity, which is crucial for pharmaceutical applications. Enzymatic extraction improves starch yield by of up to 20%, reduces energy consumption by about 30%, and lowers wastewater production by up to 50%, compared to conventional methods. Sustainable starch modification can reduce the carbon footprint of starch production by up to 40%. Modified starches contribute to approximately 70% of the food texturizers market. The market of starch in plant-based meat alternatives has grown by over 30% in the past five years. Similarly, the use of biodegradable starch-based plastics by the bioplastic industry is growing over 20% annually, driven by the demand for sustainable packaging.Kindly check and confirm the layout of Table 1.Layout is right"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Plant starch extraction, modification, and green applications: a review\n",
      "URL: http://dx.doi.org/10.1007/s10311-024-01753-z\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 15):\n",
    "    if type(data[i]['meta_data']['abstract']['p']) == list:\n",
    "        single_string_abstract = \" \".join(data[i]['meta_data']['abstract']['p'])\n",
    "        display(Markdown(f\"**{i}. Abstract**\\n\\n{single_string_abstract}\"))\n",
    "    else:\n",
    "        display(Markdown(f\"**{i}. Abstract**\\n\\n{data[i]['meta_data']['abstract']['p']}\"))\n",
    "        \n",
    "    # we'll visit the paper ourselves and find some good facts or results to create questions around\n",
    "    print(\"Title:\", data[i]['meta_data']['title'])\n",
    "    print(\"URL:\", data[i]['meta_data']['url'][0]['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at these papers manually we can get some good user context and questions that are relevant to these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_me_1 = \"I am obese and just got the weight loss surgery done. I need to increase my protein intake bu can't tolerate dairy well.\"\n",
    "question_1 = \"What is the most tolerable protein enhancing strategy for someone like me?\"\n",
    "\n",
    "about_me_2 = \"I am obese am considering getting the weight loss surgery.\"\n",
    "question_2 = \"Is there any major concerns dietary wise?\"\n",
    "\n",
    "about_me_3 = \"I am an old Chinese adult and am at risk of falling often.\"\n",
    "question_3 = \"Is there a specific type of diet that might help in preventing me fall?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 100 Similar Samples\n",
    "\n",
    "Now we'll use OpenAI to generate 100 similar samples from the above examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Here are 3 user contexts and questions about science-based research on plant-based health and nutrition. \n",
    "Generate 100 more samples like them:\n",
    "\n",
    "# about_me_1\n",
    "{about_me_1}\n",
    "\n",
    "# question_1\n",
    "{question_1}\n",
    "\n",
    "# about_me_2\n",
    "{about_me_2}\n",
    "\n",
    "# question_2\n",
    "{question_2}\n",
    "\n",
    "# about_me_3\n",
    "{about_me_3}\n",
    "\n",
    "# question_3\n",
    "{question_3}\n",
    "\n",
    "And put them in a JSON format with the keys: about_me and question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are 3 user contexts and questions about science-based research on plant-based health and nutrition. \n",
      "Generate 100 more samples like them:\n",
      "\n",
      "# about_me_1\n",
      "I am obese and just got the weight loss surgery done. I need to increase my protein intake bu can't tolerate dairy well.\n",
      "\n",
      "# question_1\n",
      "What is the most tolerable protein enhancing strategy for someone like me?\n",
      "\n",
      "# about_me_2\n",
      "I am obese am considering getting the weight loss surgery.\n",
      "\n",
      "# question_2\n",
      "Is there any major concerns dietary wise?\n",
      "\n",
      "# about_me_3\n",
      "I am an old Chinese adult and am at risk of falling often.\n",
      "\n",
      "# question_3\n",
      "Is there a specific type of diet that might help in preventing me fall?\n",
      "\n",
      "And put them in a JSON format with the keys: about_me and question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PROMPT_TEMPLATE.format(\n",
    "    about_me_1=about_me_1,\n",
    "    question_1=question_1,\n",
    "    about_me_2=about_me_2,\n",
    "    question_2=question_2,\n",
    "    about_me_3=about_me_3,\n",
    "    question_3=question_3\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=open_api_key,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"about_me\": \"I am obese and just got the weight loss surgery done. I need to increase my protein intake but can't tolerate dairy well.\",\n",
      "      \"question\": \"What is the most tolerable protein enhancing strategy for someone like me?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am obese and am considering getting weight loss surgery.\",\n",
      "      \"question\": \"Is there any major concerns dietary wise?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am an old Chinese adult and am at risk of falling often.\",\n",
      "      \"question\": \"Is there a specific type of diet that might help prevent me from falling?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a vegan athlete looking to optimize my performance.\",\n",
      "      \"question\": \"What plant-based foods can help improve athletic performance?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have a family history of heart disease and want to improve my heart health.\",\n",
      "      \"question\": \"What plant-based diet is recommended for improving heart health?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am pregnant and following a plant-based diet.\",\n",
      "      \"question\": \"How can I ensure I am getting all the necessary nutrients for a healthy pregnancy?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a student looking to improve my concentration and focus through diet.\",\n",
      "      \"question\": \"What plant-based foods are known to enhance cognitive function?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have irritable bowel syndrome (IBS) and am looking for a plant-based diet that can help manage my symptoms.\",\n",
      "      \"question\": \"Are there specific plant-based foods that can ease IBS symptoms?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a competitive runner interested in plant-based nutrition to enhance my training.\",\n",
      "      \"question\": \"What dietary strategies can help improve my running performance on a plant-based diet?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a parent looking to transition my family to a more plant-based diet.\",\n",
      "      \"question\": \"How can I ensure my family is getting all the nutrients they need on a plant-based diet?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have diabetes and am interested in plant-based meal options to help manage my blood sugar levels.\",\n",
      "      \"question\": \"What plant-based meals are recommended for individuals with diabetes?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a cancer survivor looking to optimize my diet for overall health and wellness.\",\n",
      "      \"question\": \"What plant-based foods are known for their anti-inflammatory properties that may benefit cancer survivors?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a new vegan and am concerned about getting enough protein in my diet.\",\n",
      "      \"question\": \"What plant-based sources of protein should I incorporate into my meals?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have high cholesterol and am interested in plant-based dietary options to help lower it.\",\n",
      "      \"question\": \"What plant-based foods can help lower cholesterol levels?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a vegetarian looking to switch to a fully plant-based diet for ethical reasons.\",\n",
      "      \"question\": \"How can I ensure I am meeting all my nutrient needs on a plant-based diet?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am lactose intolerant and want to explore plant-based milk alternatives.\",\n",
      "      \"question\": \"What are some plant-based milk options that are lactose-free?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a chef interested in incorporating more plant-based dishes into my menu.\",\n",
      "      \"question\": \"What are some creative plant-based recipes that appeal to a wide range of palates?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a bodybuilder looking to build muscle on a plant-based diet.\",\n",
      "      \"question\": \"What plant-based protein sources are best for muscle building?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a child with food allergies and am looking for plant-based alternatives.\",\n",
      "      \"question\": \"What plant-based foods can be good substitutes for common allergens like dairy and nuts?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have arthritis and am interested in plant-based foods that may help reduce inflammation.\",\n",
      "      \"question\": \"Are there specific plant-based foods that are known to have anti-inflammatory properties for arthritis sufferers?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am an athlete recovering from an injury and want to optimize my diet for healing.\",\n",
      "      \"question\": \"What plant-based foods can aid in the recovery process from sports injuries?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a student looking to improve my skin health through diet.\",\n",
      "      \"question\": \"What plant-based foods are beneficial for promoting healthy skin?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a new mother considering a plant-based diet for my baby.\",\n",
      "      \"question\": \"Is a plant-based diet safe and nutritious for infants?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a senior citizen looking to improve my gut health.\",\n",
      "      \"question\": \"Are there plant-based foods that can promote gut health in older adults?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a healthcare professional interested in the benefits of plant-based nutrition for patients.\",\n",
      "      \"question\": \"What scientific evidence supports the use of plant-based diets for improving health outcomes?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a college student with a busy schedule and am looking for plant-based meal prep ideas.\",\n",
      "      \"question\": \"What are some quick and easy plant-based meals that can be prepped ahead of time?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am training for a marathon and want to ensure I am fueling my body with the right plant-based foods.\",\n",
      "      \"question\": \"What plant-based foods can help with endurance and recovery during marathon training?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have celiac disease and am interested in plant-based gluten-free options.\",\n",
      "      \"question\": \"What plant-based foods are naturally gluten-free and safe for individuals with celiac disease?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a chef specializing in plant-based cuisine and am looking for unique ingredients to work with.\",\n",
      "      \"question\": \"What are some lesser-known plant-based ingredients that can add flavor and variety to dishes?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a teacher planning a nutrition lesson for my students and want to focus on plant-based eating.\",\n",
      "      \"question\": \"What are the key nutrients to highlight when teaching about plant-based nutrition to students?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have a gluten intolerance and am seeking plant-based options for a balanced diet.\",\n",
      "      \"question\": \"What plant-based foods are naturally gluten-free and can provide essential nutrients for those with gluten intolerance?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am an athlete looking to improve my recovery time after workouts with plant-based foods.\",\n",
      "      \"question\": \"What plant-based foods can aid in faster muscle recovery post-exercise?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a nutritionist specializing in plant-based diets and am interested in the latest research on their health benefits.\",\n",
      "      \"question\": \"What recent studies have been published on the health advantages of plant-based nutrition?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a parent of a picky eater and am looking for creative ways to incorporate more plant-based foods into their diet.\",\n",
      "      \"question\": \"What are some fun and engaging plant-based recipes that might appeal to picky eaters?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a healthcare provider working with patients with chronic conditions and am considering recommending a plant-based diet.\",\n",
      "      \"question\": \"What are the potential benefits of plant-based eating for individuals with chronic illnesses?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a yoga instructor interested in how plant-based nutrition can complement a healthy lifestyle.\",\n",
      "      \"question\": \"How can plant-based foods support overall wellness and enhance yoga practice?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a personal trainer advising clients on weight loss and am considering incorporating plant-based meal plans.\",\n",
      "      \"question\": \"What plant-based meal options can be effective for weight loss and muscle building?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a nutrition student researching the effects of plant-based diets on mental health.\",\n",
      "      \"question\": \"What is the connection between plant-based nutrition and mental well-being?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have a family history of diabetes and am interested in preventing the disease through plant-based nutrition.\",\n",
      "      \"question\": \"How can plant-based eating help reduce the risk of developing diabetes?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a chef creating a plant-based menu for a special event and want to showcase a variety of flavors and textures.\",\n",
      "      \"question\": \"What plant-based dishes can I prepare to offer guests a diverse and satisfying culinary experience?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a fitness enthusiast looking to incorporate more plant-based foods to support my workout routine.\",\n",
      "      \"question\": \"How can plant-based nutrition optimize performance and recovery for exercise enthusiasts?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a nature lover interested in sustainable eating practices and am considering a plant-based diet.\",\n",
      "      \"question\": \"How does plant-based nutrition contribute to environmental sustainability and conservation efforts?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a traveler exploring different cultures and cuisines and am curious about plant-based options around the world.\",\n",
      "      \"question\": \"What are some traditional plant-based dishes from various countries that I can experience while traveling?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a food blogger looking to create plant-based recipes that are both visually appealing and nutritious.\",\n",
      "      \"question\": \"What are some visually stunning plant-based dishes that are also packed with essential nutrients?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a mental health professional interested in the role of nutrition in mood disorders and am considering plant-based options for my clients.\",\n",
      "      \"question\": \"How can plant-based eating influence mood and emotional well-being in individuals with mental health concerns?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a chef specializing in plant-based cuisine and am looking for innovative ways to incorporate more protein into my dishes.\",\n",
      "      \"question\": \"What plant-based protein sources can I use to create high-protein meals for my customers?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a fitness coach working with clients to improve their overall health and am considering plant-based meal plans as part of their regimen.\",\n",
      "      \"question\": \"How can plant-based nutrition support weight management and cardiovascular health in my clients?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a food scientist researching plant-based alternatives to common allergens.\",\n",
      "      \"question\": \"What plant-based ingredients can be used as substitutes for common food allergens like dairy, eggs, and nuts?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am an athlete recovering from a sports-related injury and am looking for plant-based foods to aid in the healing process.\",\n",
      "      \"question\": \"What plant-based foods can help speed up recovery from sports injuries and improve overall healing?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a physician interested in incorporating more plant-based recommendations into my practice for preventive care.\",\n",
      "      \"question\": \"What evidence supports the use of plant-based diets for preventing chronic diseases in patients?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have a busy lifestyle and am looking for plant-based meal prep tips to save time during the week.\",\n",
      "      \"question\": \"How can I streamline meal preparation with plant-based ingredients for quick and easy meals?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a nutritionist working with clients who have digestive issues and am considering plant-based solutions to improve gut health.\",\n",
      "      \"question\": \"What plant-based foods are beneficial for promoting a healthy digestive system and gut microbiome?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a professional athlete looking to optimize my performance through plant-based nutrition.\",\n",
      "      \"question\": \"What plant-based meal plans can help me achieve peak athletic performance and recovery?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a culinary student exploring plant-based cuisine and am interested in traditional plant-based dishes from different cultures.\",\n",
      "      \"question\": \"What are some classic plant-based recipes from around the world that I can study and recreate?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a wellness coach specializing in plant-based nutrition and am looking for resources to educate my clients on its benefits.\",\n",
      "      \"question\": \"What are some reputable sources of information on plant-based eating for educating clients about its advantages?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a gardener interested in growing my own plant-based ingredients and am looking for advice on sustainable gardening practices.\",\n",
      "      \"question\": \"How can I cultivate a garden that produces a variety of plant-based foods year-round and supports a sustainable lifestyle?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a food entrepreneur developing a plant-based product line and am seeking guidance on ingredient selection.\",\n",
      "      \"question\": \"What plant-based ingredients are trending in the food industry and can enhance the appeal of my product line?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have osteoporosis and am considering plant-based dietary options to promote bone health.\",\n",
      "      \"question\": \"Are there specific plant-based foods that can help improve bone density and reduce the risk of fractures in individuals with osteoporosis?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a chef looking to create plant-based meals that are high in fiber and promote digestive health.\",\n",
      "      \"question\": \"What high-fiber plant-based ingredients can I use to develop recipes that support gut health and regular digestion?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I have high blood pressure and am interested in plant-based meal options to help lower it.\",\n",
      "      \"question\": \"What plant-based foods can be effective in reducing blood pressure and promoting heart health?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a nutrition educator conducting workshops on plant-based eating and am seeking engaging activities for participants.\",\n",
      "      \"question\": \"What interactive exercises and demonstrations can I include in my workshops to educate individuals on the benefits of plant-based nutrition?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a parent of a teenage athlete and am looking for plant-based meal ideas to support their training and performance.\",\n",
      "      \"question\": \"What nutrient-dense plant-based meals can I prepare for my teenage athlete to optimize their energy and recovery?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a restaurant owner interested in adding more plant-based options to my menu and am looking for guidance on creating plant-based dishes that appeal to a diverse clientele.\",\n",
      "      \"question\": \"How can I develop plant-based recipes that cater to different dietary preferences and taste preferences of customers?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a sustainability advocate passionate about reducing food waste and am seeking plant-based solutions to minimize environmental impact.\",\n",
      "      \"question\": \"What are some creative ways to use plant-based ingredients to create zero-waste dishes and contribute to sustainable food practices?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a corporate wellness coordinator planning a plant-based challenge for employees and am looking for resources to support their participation.\",\n",
      "      \"question\": \"What educational materials and meal plans can I provide to employees to make a plant-based challenge successful and engaging?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a dietitian working with clients interested in transitioning to a plant-based lifestyle and am seeking evidence-based recommendations to guide them through the process.\",\n",
      "      \"question\": \"What are the key steps and considerations for individuals looking to adopt a plant-based diet for improved health and well-being?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a baker experimenting with plant-based desserts and am looking for innovative ways to substitute traditional ingredients.\",\n",
      "      \"question\": \"What plant-based alternatives can I use to create decadent and flavorful desserts without dairy or eggs?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a fitness blogger interested in the connection between plant-based nutrition and physical performance and recovery.\",\n",
      "      \"question\": \"How does plant-based eating impact exercise performance, muscle repair, and recovery for athletes and active individuals?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a restaurant chef interested in developing a plant-based menu that caters to customers with various dietary restrictions.\",\n",
      "      \"question\": \"How can I create a diverse and inclusive plant-based menu that accommodates patrons with allergies, intolerances, and special dietary needs?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a culinary instructor teaching plant-based cooking classes and am looking for theme night ideas to engage students in different cuisines.\",\n",
      "      \"question\": \"What are some creative themes for plant-based cooking classes that can introduce students to global flavors and culinary traditions?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a nutrition researcher studying the health benefits of plant-based diets and am interested in the long-term effects on chronic disease prevention.\",\n",
      "      \"question\": \"What long-term studies have been conducted on the impact of plant-based nutrition on reducing the risk of chronic diseases such as heart disease, diabetes, and cancer?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a wellness retreat organizer planning a plant-based retreat and am seeking guidance on creating a balanced and nourishing menu for participants.\",\n",
      "      \"question\": \"What plant-based meal options can I offer at the retreat that are not only delicious but also provide essential nutrients for overall well-being?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a food writer researching plant-based trends and am looking for insights on emerging ingredients and flavors in plant-based cuisine.\",\n",
      "      \"question\": \"What new plant-based ingredients and flavor profiles are gaining popularity in the culinary world and shaping the future of plant-based eating?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a cooking show host interested in featuring more plant-based recipes and am looking for culinary experts to share their plant-based cooking tips and techniques.\",\n",
      "      \"question\": \"How can I showcase diverse plant-based recipes and cooking methods through guest chefs on my show to inspire viewers to try plant-based dishes at home?\"\n",
      "    },\n",
      "    {\n",
      "      \"about_me\": \"I am a food blogger exploring the versatility of plant-based ingredients in global cuisines and am seeking inspiration for new plant-based recipes.\",\n",
      "      \"question\": \"What are some traditional plant-based dishes from different countries that I can adapt and reimagine with a modern twist for my readers?\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "synthetic_data = chat_completion.choices[0].message.content\n",
    "print(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = json.loads(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this data, let's save it into a config file so we can load it whenever we need it. We will just copy and paste it since it is a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'about_me': \"I am obese and just got the weight loss surgery done. I need to increase my protein intake but can't tolerate dairy well.\",\n",
       "  'question': 'What is the most tolerable protein enhancing strategy for someone like me?'},\n",
       " {'about_me': 'I am obese and am considering getting weight loss surgery.',\n",
       "  'question': 'Is there any major concerns dietary wise?'},\n",
       " {'about_me': 'I am an old Chinese adult and am at risk of falling often.',\n",
       "  'question': 'Is there a specific type of diet that might help prevent me from falling?'},\n",
       " {'about_me': 'I am a vegan athlete looking to optimize my performance.',\n",
       "  'question': 'What plant-based foods can help improve athletic performance?'},\n",
       " {'about_me': 'I have a family history of heart disease and want to improve my heart health.',\n",
       "  'question': 'What plant-based diet is recommended for improving heart health?'},\n",
       " {'about_me': 'I am pregnant and following a plant-based diet.',\n",
       "  'question': 'How can I ensure I am getting all the necessary nutrients for a healthy pregnancy?'},\n",
       " {'about_me': 'I am a student looking to improve my concentration and focus through diet.',\n",
       "  'question': 'What plant-based foods are known to enhance cognitive function?'},\n",
       " {'about_me': 'I have irritable bowel syndrome (IBS) and am looking for a plant-based diet that can help manage my symptoms.',\n",
       "  'question': 'Are there specific plant-based foods that can ease IBS symptoms?'},\n",
       " {'about_me': 'I am a competitive runner interested in plant-based nutrition to enhance my training.',\n",
       "  'question': 'What dietary strategies can help improve my running performance on a plant-based diet?'},\n",
       " {'about_me': 'I am a parent looking to transition my family to a more plant-based diet.',\n",
       "  'question': 'How can I ensure my family is getting all the nutrients they need on a plant-based diet?'},\n",
       " {'about_me': 'I have diabetes and am interested in plant-based meal options to help manage my blood sugar levels.',\n",
       "  'question': 'What plant-based meals are recommended for individuals with diabetes?'},\n",
       " {'about_me': 'I am a cancer survivor looking to optimize my diet for overall health and wellness.',\n",
       "  'question': 'What plant-based foods are known for their anti-inflammatory properties that may benefit cancer survivors?'},\n",
       " {'about_me': 'I am a new vegan and am concerned about getting enough protein in my diet.',\n",
       "  'question': 'What plant-based sources of protein should I incorporate into my meals?'},\n",
       " {'about_me': 'I have high cholesterol and am interested in plant-based dietary options to help lower it.',\n",
       "  'question': 'What plant-based foods can help lower cholesterol levels?'},\n",
       " {'about_me': 'I am a vegetarian looking to switch to a fully plant-based diet for ethical reasons.',\n",
       "  'question': 'How can I ensure I am meeting all my nutrient needs on a plant-based diet?'},\n",
       " {'about_me': 'I am lactose intolerant and want to explore plant-based milk alternatives.',\n",
       "  'question': 'What are some plant-based milk options that are lactose-free?'},\n",
       " {'about_me': 'I am a chef interested in incorporating more plant-based dishes into my menu.',\n",
       "  'question': 'What are some creative plant-based recipes that appeal to a wide range of palates?'},\n",
       " {'about_me': 'I am a bodybuilder looking to build muscle on a plant-based diet.',\n",
       "  'question': 'What plant-based protein sources are best for muscle building?'},\n",
       " {'about_me': 'I am a child with food allergies and am looking for plant-based alternatives.',\n",
       "  'question': 'What plant-based foods can be good substitutes for common allergens like dairy and nuts?'},\n",
       " {'about_me': 'I have arthritis and am interested in plant-based foods that may help reduce inflammation.',\n",
       "  'question': 'Are there specific plant-based foods that are known to have anti-inflammatory properties for arthritis sufferers?'},\n",
       " {'about_me': 'I am an athlete recovering from an injury and want to optimize my diet for healing.',\n",
       "  'question': 'What plant-based foods can aid in the recovery process from sports injuries?'},\n",
       " {'about_me': 'I am a student looking to improve my skin health through diet.',\n",
       "  'question': 'What plant-based foods are beneficial for promoting healthy skin?'},\n",
       " {'about_me': 'I am a new mother considering a plant-based diet for my baby.',\n",
       "  'question': 'Is a plant-based diet safe and nutritious for infants?'},\n",
       " {'about_me': 'I am a senior citizen looking to improve my gut health.',\n",
       "  'question': 'Are there plant-based foods that can promote gut health in older adults?'},\n",
       " {'about_me': 'I am a healthcare professional interested in the benefits of plant-based nutrition for patients.',\n",
       "  'question': 'What scientific evidence supports the use of plant-based diets for improving health outcomes?'},\n",
       " {'about_me': 'I am a college student with a busy schedule and am looking for plant-based meal prep ideas.',\n",
       "  'question': 'What are some quick and easy plant-based meals that can be prepped ahead of time?'},\n",
       " {'about_me': 'I am training for a marathon and want to ensure I am fueling my body with the right plant-based foods.',\n",
       "  'question': 'What plant-based foods can help with endurance and recovery during marathon training?'},\n",
       " {'about_me': 'I have celiac disease and am interested in plant-based gluten-free options.',\n",
       "  'question': 'What plant-based foods are naturally gluten-free and safe for individuals with celiac disease?'},\n",
       " {'about_me': 'I am a chef specializing in plant-based cuisine and am looking for unique ingredients to work with.',\n",
       "  'question': 'What are some lesser-known plant-based ingredients that can add flavor and variety to dishes?'},\n",
       " {'about_me': 'I am a teacher planning a nutrition lesson for my students and want to focus on plant-based eating.',\n",
       "  'question': 'What are the key nutrients to highlight when teaching about plant-based nutrition to students?'},\n",
       " {'about_me': 'I have a gluten intolerance and am seeking plant-based options for a balanced diet.',\n",
       "  'question': 'What plant-based foods are naturally gluten-free and can provide essential nutrients for those with gluten intolerance?'},\n",
       " {'about_me': 'I am an athlete looking to improve my recovery time after workouts with plant-based foods.',\n",
       "  'question': 'What plant-based foods can aid in faster muscle recovery post-exercise?'},\n",
       " {'about_me': 'I am a nutritionist specializing in plant-based diets and am interested in the latest research on their health benefits.',\n",
       "  'question': 'What recent studies have been published on the health advantages of plant-based nutrition?'},\n",
       " {'about_me': 'I am a parent of a picky eater and am looking for creative ways to incorporate more plant-based foods into their diet.',\n",
       "  'question': 'What are some fun and engaging plant-based recipes that might appeal to picky eaters?'},\n",
       " {'about_me': 'I am a healthcare provider working with patients with chronic conditions and am considering recommending a plant-based diet.',\n",
       "  'question': 'What are the potential benefits of plant-based eating for individuals with chronic illnesses?'},\n",
       " {'about_me': 'I am a yoga instructor interested in how plant-based nutrition can complement a healthy lifestyle.',\n",
       "  'question': 'How can plant-based foods support overall wellness and enhance yoga practice?'},\n",
       " {'about_me': 'I am a personal trainer advising clients on weight loss and am considering incorporating plant-based meal plans.',\n",
       "  'question': 'What plant-based meal options can be effective for weight loss and muscle building?'},\n",
       " {'about_me': 'I am a nutrition student researching the effects of plant-based diets on mental health.',\n",
       "  'question': 'What is the connection between plant-based nutrition and mental well-being?'},\n",
       " {'about_me': 'I have a family history of diabetes and am interested in preventing the disease through plant-based nutrition.',\n",
       "  'question': 'How can plant-based eating help reduce the risk of developing diabetes?'},\n",
       " {'about_me': 'I am a chef creating a plant-based menu for a special event and want to showcase a variety of flavors and textures.',\n",
       "  'question': 'What plant-based dishes can I prepare to offer guests a diverse and satisfying culinary experience?'},\n",
       " {'about_me': 'I am a fitness enthusiast looking to incorporate more plant-based foods to support my workout routine.',\n",
       "  'question': 'How can plant-based nutrition optimize performance and recovery for exercise enthusiasts?'},\n",
       " {'about_me': 'I am a nature lover interested in sustainable eating practices and am considering a plant-based diet.',\n",
       "  'question': 'How does plant-based nutrition contribute to environmental sustainability and conservation efforts?'},\n",
       " {'about_me': 'I am a traveler exploring different cultures and cuisines and am curious about plant-based options around the world.',\n",
       "  'question': 'What are some traditional plant-based dishes from various countries that I can experience while traveling?'},\n",
       " {'about_me': 'I am a food blogger looking to create plant-based recipes that are both visually appealing and nutritious.',\n",
       "  'question': 'What are some visually stunning plant-based dishes that are also packed with essential nutrients?'},\n",
       " {'about_me': 'I am a mental health professional interested in the role of nutrition in mood disorders and am considering plant-based options for my clients.',\n",
       "  'question': 'How can plant-based eating influence mood and emotional well-being in individuals with mental health concerns?'},\n",
       " {'about_me': 'I am a chef specializing in plant-based cuisine and am looking for innovative ways to incorporate more protein into my dishes.',\n",
       "  'question': 'What plant-based protein sources can I use to create high-protein meals for my customers?'},\n",
       " {'about_me': 'I am a fitness coach working with clients to improve their overall health and am considering plant-based meal plans as part of their regimen.',\n",
       "  'question': 'How can plant-based nutrition support weight management and cardiovascular health in my clients?'},\n",
       " {'about_me': 'I am a food scientist researching plant-based alternatives to common allergens.',\n",
       "  'question': 'What plant-based ingredients can be used as substitutes for common food allergens like dairy, eggs, and nuts?'},\n",
       " {'about_me': 'I am an athlete recovering from a sports-related injury and am looking for plant-based foods to aid in the healing process.',\n",
       "  'question': 'What plant-based foods can help speed up recovery from sports injuries and improve overall healing?'},\n",
       " {'about_me': 'I am a physician interested in incorporating more plant-based recommendations into my practice for preventive care.',\n",
       "  'question': 'What evidence supports the use of plant-based diets for preventing chronic diseases in patients?'},\n",
       " {'about_me': 'I have a busy lifestyle and am looking for plant-based meal prep tips to save time during the week.',\n",
       "  'question': 'How can I streamline meal preparation with plant-based ingredients for quick and easy meals?'},\n",
       " {'about_me': 'I am a nutritionist working with clients who have digestive issues and am considering plant-based solutions to improve gut health.',\n",
       "  'question': 'What plant-based foods are beneficial for promoting a healthy digestive system and gut microbiome?'},\n",
       " {'about_me': 'I am a professional athlete looking to optimize my performance through plant-based nutrition.',\n",
       "  'question': 'What plant-based meal plans can help me achieve peak athletic performance and recovery?'},\n",
       " {'about_me': 'I am a culinary student exploring plant-based cuisine and am interested in traditional plant-based dishes from different cultures.',\n",
       "  'question': 'What are some classic plant-based recipes from around the world that I can study and recreate?'},\n",
       " {'about_me': 'I am a wellness coach specializing in plant-based nutrition and am looking for resources to educate my clients on its benefits.',\n",
       "  'question': 'What are some reputable sources of information on plant-based eating for educating clients about its advantages?'},\n",
       " {'about_me': 'I am a gardener interested in growing my own plant-based ingredients and am looking for advice on sustainable gardening practices.',\n",
       "  'question': 'How can I cultivate a garden that produces a variety of plant-based foods year-round and supports a sustainable lifestyle?'},\n",
       " {'about_me': 'I am a food entrepreneur developing a plant-based product line and am seeking guidance on ingredient selection.',\n",
       "  'question': 'What plant-based ingredients are trending in the food industry and can enhance the appeal of my product line?'},\n",
       " {'about_me': 'I have osteoporosis and am considering plant-based dietary options to promote bone health.',\n",
       "  'question': 'Are there specific plant-based foods that can help improve bone density and reduce the risk of fractures in individuals with osteoporosis?'},\n",
       " {'about_me': 'I am a chef looking to create plant-based meals that are high in fiber and promote digestive health.',\n",
       "  'question': 'What high-fiber plant-based ingredients can I use to develop recipes that support gut health and regular digestion?'},\n",
       " {'about_me': 'I have high blood pressure and am interested in plant-based meal options to help lower it.',\n",
       "  'question': 'What plant-based foods can be effective in reducing blood pressure and promoting heart health?'},\n",
       " {'about_me': 'I am a nutrition educator conducting workshops on plant-based eating and am seeking engaging activities for participants.',\n",
       "  'question': 'What interactive exercises and demonstrations can I include in my workshops to educate individuals on the benefits of plant-based nutrition?'},\n",
       " {'about_me': 'I am a parent of a teenage athlete and am looking for plant-based meal ideas to support their training and performance.',\n",
       "  'question': 'What nutrient-dense plant-based meals can I prepare for my teenage athlete to optimize their energy and recovery?'},\n",
       " {'about_me': 'I am a restaurant owner interested in adding more plant-based options to my menu and am looking for guidance on creating plant-based dishes that appeal to a diverse clientele.',\n",
       "  'question': 'How can I develop plant-based recipes that cater to different dietary preferences and taste preferences of customers?'},\n",
       " {'about_me': 'I am a sustainability advocate passionate about reducing food waste and am seeking plant-based solutions to minimize environmental impact.',\n",
       "  'question': 'What are some creative ways to use plant-based ingredients to create zero-waste dishes and contribute to sustainable food practices?'},\n",
       " {'about_me': 'I am a corporate wellness coordinator planning a plant-based challenge for employees and am looking for resources to support their participation.',\n",
       "  'question': 'What educational materials and meal plans can I provide to employees to make a plant-based challenge successful and engaging?'},\n",
       " {'about_me': 'I am a dietitian working with clients interested in transitioning to a plant-based lifestyle and am seeking evidence-based recommendations to guide them through the process.',\n",
       "  'question': 'What are the key steps and considerations for individuals looking to adopt a plant-based diet for improved health and well-being?'},\n",
       " {'about_me': 'I am a baker experimenting with plant-based desserts and am looking for innovative ways to substitute traditional ingredients.',\n",
       "  'question': 'What plant-based alternatives can I use to create decadent and flavorful desserts without dairy or eggs?'},\n",
       " {'about_me': 'I am a fitness blogger interested in the connection between plant-based nutrition and physical performance and recovery.',\n",
       "  'question': 'How does plant-based eating impact exercise performance, muscle repair, and recovery for athletes and active individuals?'},\n",
       " {'about_me': 'I am a restaurant chef interested in developing a plant-based menu that caters to customers with various dietary restrictions.',\n",
       "  'question': 'How can I create a diverse and inclusive plant-based menu that accommodates patrons with allergies, intolerances, and special dietary needs?'},\n",
       " {'about_me': 'I am a culinary instructor teaching plant-based cooking classes and am looking for theme night ideas to engage students in different cuisines.',\n",
       "  'question': 'What are some creative themes for plant-based cooking classes that can introduce students to global flavors and culinary traditions?'},\n",
       " {'about_me': 'I am a nutrition researcher studying the health benefits of plant-based diets and am interested in the long-term effects on chronic disease prevention.',\n",
       "  'question': 'What long-term studies have been conducted on the impact of plant-based nutrition on reducing the risk of chronic diseases such as heart disease, diabetes, and cancer?'},\n",
       " {'about_me': 'I am a wellness retreat organizer planning a plant-based retreat and am seeking guidance on creating a balanced and nourishing menu for participants.',\n",
       "  'question': 'What plant-based meal options can I offer at the retreat that are not only delicious but also provide essential nutrients for overall well-being?'},\n",
       " {'about_me': 'I am a food writer researching plant-based trends and am looking for insights on emerging ingredients and flavors in plant-based cuisine.',\n",
       "  'question': 'What new plant-based ingredients and flavor profiles are gaining popularity in the culinary world and shaping the future of plant-based eating?'},\n",
       " {'about_me': 'I am a cooking show host interested in featuring more plant-based recipes and am looking for culinary experts to share their plant-based cooking tips and techniques.',\n",
       "  'question': 'How can I showcase diverse plant-based recipes and cooking methods through guest chefs on my show to inspire viewers to try plant-based dishes at home?'},\n",
       " {'about_me': 'I am a food blogger exploring the versatility of plant-based ingredients in global cuisines and am seeking inspiration for new plant-based recipes.',\n",
       "  'question': 'What are some traditional plant-based dishes from different countries that I can adapt and reimagine with a modern twist for my readers?'}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Outputs with RAG\n",
    "\n",
    "Next we need to generate responses to all these questions. But to make them relevant we will use RAG on our pdf data to get relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RaviB\\\\GitHub\\\\vegan-ai-nutritionist\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check current directory with above line and here change it to root\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in the examples that we saved manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from modules.q_and_a_dataset.src.examples import EXAMPLES\n",
    "from modules.data_processing.src.config import INDEX_NAME, EMBEDDING_MODEL_ID\n",
    "from modules.data_processing.src.embeddings import get_embedding_model, generate_embeddings\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "opensearch_endpoint = os.environ.get('OPENSEARCH_ENDPOINT')\n",
    "\n",
    "AWS_ACCESS_KEY = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_REGION = os.environ.get('AWS_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'b618476fdb5879478fc667c4fb6cd473', 'cluster_name': '590184030535:vegan-pdf-data', 'cluster_uuid': 'D2l3HY8VSk-qIbco5LH9gg', 'version': {'distribution': 'opensearch', 'number': '2.5.0', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2024-05-02T06:25:23.555552Z', 'build_snapshot': False, 'lucene_version': '9.4.2', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
     ]
    }
   ],
   "source": [
    "awsauth = AWS4Auth(AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_REGION, 'es')\n",
    "\n",
    "# Create the OpenSearch client\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': opensearch_endpoint, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "info = client.info()\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = get_embedding_model(EMBEDDING_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will concatenate the about me and question text to find relevant papers to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = EXAMPLES[15]['about_me'] + ' ' + EXAMPLES[15]['question']\n",
    "query_embedding = embedding_model.encode(query_text)\n",
    "query_text += \" I also want to be vegan and can't stand the thought of animal suffering. But my main concern is still that I can drink milk with no health issues.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am lactose intolerant and want to explore plant-based milk alternatives. What are some plant-based milk options that are lactose-free? I also want to be vegan and can't stand the thought of animal suffering. But my main concern is still that I can drink milk with no health issues.\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_body = {\n",
    "    \"size\": 10,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"embedding\": {\n",
    "                \"vector\": query_embedding,\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"text\", \"metadata\"]\n",
    "}\n",
    "\n",
    "response = client.search(index=INDEX_NAME, body=search_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.42665416\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: BSH activities were observed in The differences in BSH activities of LAB isolates might be due to the influences of disparate environments on expressions of BSH genes (Song et al., Different vegetable powders including cherry tomato, beetroot, pumpkin, bok choy, and corn at 5% (w/v) were added into culture media of The viability of probiotics Moreover, addition of vegetable powders reduced probiotic deaths and stabilized the growth rates of probiotics during the stationary phase (Fig. This study was the first study which integrated corn powder into the mixture of microcapsule material. Several studies have reported prebiotic effects of corn compositions. Consumption of SCF for two weeks significantly enhanced bifidobacterial numbers in healthy volunteers (Costabile et al., Before freeze drying, there were approximately 10The survival of probiotic free cells and encapsulated of probiotics before–after freeze drying. WPI and MD have been wildly used as a mixture for microencapsulation because of their Maillard reaction’s property. Corn which is often used as a raw material in many food industries due to its low cost has never been used as encapsulate material. Structure of corn powder is composed of carbohydrates, soluble corn fiber and fibers which are suitable to be used as a wall material. Thus, incorporation of corn powder into the mixture of WPI-MD might strengthen encapsulated surface. It has been reported that chemically modified whey protein and cornstarch formulated into resistant starch (RS) used as an encapsulated material exhibited significantly greater microencapsulation property and protection against environmental microbial destruction than RS alone (Free cells and microencapsulated probiotics were observed under a scanning electron microscopy (SEM) (Fig. The morphology of probiotics microencapsulated. Scanning electron microscopy (SEM) of free cells (It was the first time that this structure was studied and reported. Combination of WPI, MD, and corn formed a reticulum-like structure wrapped in another layer which potentially increasing the encapsulation efficiency (Yan et al., Survivals of microencapsulated probiotics in simulated gastrointestinal conditions were examined using artificial gastric and intestinal fluids. The numbers probiotics free cells were decreased from an initial number of 10Moreover, Table The survival of probiotic free cells and encapsulated FormulaNumber of viability (Log CFU/mL)InitialArtificial gastric fluidsArtificial intestinal fluids1 h2 h3 h4 h5 h6 h CT9.361 ± 0.059.041 ± 0.1908.602 ± 0.1028.00 ± 0.010000 M8.014 ± 0.1047.512 ± 0.1027.115 ± 0.156.815 ± 0.1056.360 ± 0.1106.12 ± 0.1076.00 ± 0.05 MW8.653 ± 0.158.929 ± 0.10510.439 ± 0.4110.243 ± 0.109.929 ± 0.049.778 ± 0.058.301 ± 0.05 MWC7.30 ± 0.157.525 ± 0.1608.452 ± 0.418.650 ± 0.1010.254 ± 0.0410.00 ± 0.059.58 ± 0.05 CT9.243 ± 0.058.653 ± 0.1908.602 ± 0.1028.00 ± 0.010000 M8.519 ± 0.1087.301 ± 0.0107.00 ± 0.0726.284 ± 0.1156.01 ± 0.3205.367 ± 0.0485.106 ± 0.005 MW9.452 ± 0.3510.20 ± 0.7810.447 ± 0.1010.778 ± 0.259.301 ± 0.1059.01 ± 0.0488.512 ± 0.110 MWC6.550 ± 0.3057.00 ± 0.1707.52 ± 0.1057.98 ± 0.2510.00 ± 0.4559.820 ± 0.4089.05 ± 0.20 CT9.278 ± 0.0678.954 ± 0.1028.176 ± 0.307.698 ± 0.145000 M8.140 ± 0.6007.845 ± 0.1257.501 ± 0.016.450 ± 0.2306.205 ± 0.1106.07 ± 0.4055.95 ± 0.085 MW9.20 ± 0.1089.929 ± 0.10810.74 ± 0.03510.45 ± 0.06510.04 ± 0.5059.653 ± 0.1409.107 ± 0.750 MWC6.507 ± 0.1087.000 ± 0.1087.250 ± 0.0357.860 ± 0.06510.658 ± 0.50510.754 ± 0.14010.25 ± 0.750Data are represented as means ± SD (n = 5)Viabilities of free cells (CT) and microencapsulated probiotics stored at 4 °C and 25 °C (room temperature) for 1 year are shown in Fig. Stability of probiotics encapsulated and free cells during long-term storage at 4 °C and 25 °C. M was MD alone. MW was the mixture of MD-WPI. MWC was the mixture of MD-WPI, and CT was free cells used as a control. Data are represented as means ± SD means (n = 5). ****Bile salt hydrolase enzyme activities were maintained in microencapsulated probiotics. These probiotics continued to produce BSH with the same potency as probiotics without microencapsulation as shown in Table  Bile salt hydrolase (BSH) enzyme activity of probiotics under each experimental conditionConditionBSH activityCTMMWMWCCTMMWMWCCTMMWMWCBefore freeze drying +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  + After freeze drying +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  + Artificial gastric\n",
      "\n",
      "Score: 0.42157134\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: Different vegetable powders including cherry tomato, beetroot, pumpkin, bok choy, and corn at 5% (w/v) were added into culture media of The viability of probiotics Moreover, addition of vegetable powders reduced probiotic deaths and stabilized the growth rates of probiotics during the stationary phase (Fig. This study was the first study which integrated corn powder into the mixture of microcapsule material. Several studies have reported prebiotic effects of corn compositions. Consumption of SCF for two weeks significantly enhanced bifidobacterial numbers in healthy volunteers (Costabile et al.,\n",
      "\n",
      "Score: 0.4147529\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: Before freeze drying, there were approximately 10The survival of probiotic free cells and encapsulated of probiotics before–after freeze drying. WPI and MD have been wildly used as a mixture for microencapsulation because of their Maillard reaction’s property. Corn which is often used as a raw material in many food industries due to its low cost has never been used as encapsulate material. Structure of corn powder is composed of carbohydrates, soluble corn fiber and fibers which are suitable to be used as a wall material. Thus, incorporation of corn powder into the mixture of WPI-MD might strengthen encapsulated surface. It has been reported that chemically modified whey protein and cornstarch formulated into resistant starch (RS) used as an encapsulated material exhibited significantly greater microencapsulation property and protection against environmental microbial destruction than RS alone (\n",
      "\n",
      "Score: 0.41194502\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: Three strains of probiotics with the ability to produce bile-salt hydrolase enzyme were selected for this study.\n",
      "\n",
      "Score: 0.40823525\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: Probiotics (10\n",
      "\n",
      "Score: 0.40263855\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: Three strains of probiotics with the ability to produce bile-salt hydrolase enzyme were selected for this study. Probiotics (10Selection of wall materials in microcapsules is important for probiotics encapsulation to stabilize viability of probiotics during difference conditions (Kambhampati et al., In this study, viabilities of probiotics in microcapsules from different materials were compared before and after the microencapsulation process by freeze drying. Three formulas of microencapsulation were compared in this study including maltodextrin alone (MD), the mixture of MD-WPI (MW), and the mixture of MD-WPI and corm powder (MWC). In addition, free cells were used as a control.The encapsulated probiotic cells and probiotic cell suspensions were used in this experiment in order to measure probiotic cells viability before and after freeze drying. The encapsulated probiotic cells were released with PBS, pH 7.2, and then all samples were 10 -fold serially diluted with PBS, pH 7.2 and spread on MRS agar with CaCOThe morphology of microcapsules was analyzed using scanning electron microscope (Hitachi High -Technologies, Japan). Microcapsule samples were prepared on stub using carbon tape and coated with gold particle for electrical conductivity. Condition of electron in vacuum was set at electric potential acceleration of 20 kV.The simulated gastric juice and simulated intestinal juice were used as a gastrointestinal condition model to study the survival of probiotics in microcapsule products after exposing to the gastrointestinal condition. Initially, the artificial gastric juice was prepared using 0.2% (w/v) sodium chloride (NaCl) solution consisting of 0.35% (w/v) pepsin (HiMedia, India), and the pH was adjusted with 1M of hydrochloric acid (HCl) to pH 2.0. The artificial intestinal juice was prepared by using 0.2% (w/v) NaCl solution consisting of 0.1% (w/v) trypsin (HiMedia, India), 1.0% (w/v) oxgall and 1.1% w/v sodium bicarbonate (NaHTo study the survivability of microencapsulated probiotics during storage, the encapsulated probiotic cells suspensions were stored at 4 °C and 25 °C for 1 years. Every month during storage, 1 g of encapsulated probiotic cells suspensions were evaluated for probiotic viability. Briefly, the encapsulated probiotic cells were counted by suspending 1 g of sample in 9 mL of PBS, pH 7.2. Then, the suspensions were tenfold serially diluted and spread on MRS agar with CaCOOne gram of encapsulated probiotic cells suspensions was evaluated for probiotic viability during storage. Briefly, the encapsulated probiotic cells were counted by suspending 1 g of sample in 9 mL of PBS, pH 7.2 and concentration at 10All experiments were performed in duplicate and repeated at least three times. The results were expressed as means ± standard deviation (n = 5) by using GraphPad program version 5.0 (San Diego, CA, USA). The Tukey’s multiple comparison with one-way ANOVA analysis were used to analyze the variance to compare the differences among various groups. A value of\n",
      "\n",
      "Score: 0.39991266\n",
      "Title: Molasses-based waste water irrigation: a friend or foe for carrot (Daucus carota L.) growth, yield and nutritional quality\n",
      "Abstract: Management of molasses-based wastewater generated in yeast and sugar industries is a major environmental concern due to its high chemical oxygen demand and other recalcitrant substances. Several strategies have been used to reduce the inland discharge of wastewater but the results are not satisfactory due to high operating cost. However, reuse of molasses-based wastewater irrigation in agriculture has been a major interest nowadays to reduce the freshwater consumption. Thus, it is crucial to monitor the impacts of molasses-based waste water irrigation on growth, metabolism, yield and nutritional quality of crops for safer consumer’s health. In present study, carrot seeds of a local cultivar (T-29) were germinated on filter paper in Petri dishes under controlled conditions. The germinated seeds were then transplanted into pots and irrigated with three different treatments normal water (T0), diluted molasses-based wastewater (T1), and untreated molasses-based wastewater (T2), in six replicates. Results revealed that carrot irrigated with untreated molasses-based waste water had exhibited significant reductions in growth, yield, physiology, metabolism, and nutritional contents. Additionally, accumulation of Cd and Pb contents in carrot roots irrigated with untreated molasses-based waste water exceed the permissible limits suggested by WHO and their consumption may cause health risks. While, diluted molasses-based waste water irrigation positively enhanced the growth, yield of carrot plants without affecting the nutritional quality. This strategy is cost effective, appeared as most appropriate alternative mean to reduce the freshwater consumption in water deficit regions of the world.\n",
      "Text: In present study, concentration of various osmoprotectans such as total soluble sugars, reducing and non-reducing sugars, total soluble proteins, and free amino acids were increased in plants exposed to untreated and diluted molasses-based waste water treatments as compared to normal water irrigation (Fig. In present study, a significant increase in activity of stress responsive enzymes including POX, PAL, SOD, CAT, NAR and NIR were observed in response to irrigation with molasses-based waste water as compared to normal water irrigation (Fig.\n",
      "\n",
      "Score: 0.39975658\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: Probiotics are beneficial commensal microbes in human. Microbiota is a large community of microorganisms that grows in the stomach, small and large intestines. The majority of these microorganisms which are beneficial to human body are members of probiotics. The definition of probiotics defined by the Food and Agriculture Organization of the United Nations/World Health Organization (FAO/WHO) with minor modified by the International Scientific Association for Probiotics and Prebiotics (ISAPP) is “live microorganisms that when administered in adequate amounts confer a health benefit on the host” (Salminen et al., Microencapsulation is an effective technology to help protect probiotics from stress conditions (Ammara et al., Corn which is abundant in Northern Thailand is a vegetable enriched with nutrients especially soluble corn fiber (SCF). SCF, a non-digestible oligosaccharide in digestive tract, is considered as prebiotics. Naturally, probiotics living in the GI tract digest this fiber into small molecules which can be absorbed into the intestine. To be classified as prebiotics, nutrients need to meet three criteria as followed. They must be digested by probiotics and are benefit to the body. They must not be digested or absorbed in the stomach and small intestine. The common prebiotics include oligosaccharide, raffinose, inulin and non-starch polysaccharides (Floch, A previous study from our group reported cholesterol lowering property of\n",
      "\n",
      "Score: 0.3991538\n",
      "Title: MicroRNAs as potent regulators in nitrogen and phosphorus signaling transduction and their applications\n",
      "Abstract: Nitrogen (N) and phosphorus (Pi) are essential macronutrients that affect plant growth and development by influencing the molecular, metabolic, biochemical, and physiological responses at the local and whole levels in plants. N and Pi stresses suppress the physiological activities of plants, resulting in agricultural productivity losses and severely threatening food security. Accordingly, plants have elaborated diverse strategies to cope with N and Pi stresses through maintaining N and Pi homeostasis. MicroRNAs (miRNAs) as potent regulators fine-tune N and Pi signaling transduction that are distinct and indivisible from each other. Specific signals, such as noncoding RNAs (ncRNAs), interact with miRNAs and add to the complexity of regulation. Elucidation of the mechanisms by which miRNAs regulate N and Pi signaling transduction aids in the breeding of plants with strong tolerance to N and Pi stresses and high N and Pi use efficiency by fine-tuning MIR genes or miRNAs. However, to date, there has been no detailed and systematic introduction and comparison of the functions of miRNAs in N and Pi signaling transduction from the perspective of miRNAs and their applications. Here, we summarized and discussed current advances in the involvement of miRNAs in N and Pi signaling transduction and highlighted that fine-tuning the MIR genes or miRNAs involved in maintaining N and Pi homeostasis might provide valuable sights for sustainable agriculture.\n",
      "Text: The function of miRNAs in maintaining N and Pi homeostasis can be explored by fine-tuning the Widely-used methods and technologies for accelerating plant breeding via fine-tuning Currently, the physiological and molecular verification and application of transgenic plants represents the most critical step for the implementation of basic research and the advancement of sustainable agriculture. Thus, we focus on discussing the technologies for fine-tuning miRNAs in transgenic research in the following (Fig. Straightforward technologies of manipulating miRNA expression are the overexpression of Meanwhile, the clustered regularly interspaced short palindromic repeats CRISPR and CRISPR/Cas system modifying DNA sequences have revolutionized biotechnology and provided genetic tools for genetically modified plants (Manghwar et al. So far, there are limitations and opportunities in the application of CRISPR/Cas system in miRNA editing breeding. The knockout is mainly based on the canonical or discontinued SpCas9 that can induce Efficient\n",
      "\n",
      "Score: 0.39671665\n",
      "Title: Freeze drying microencapsulation using whey protein, maltodextrin and corn powder improved survivability of probiotics during storage\n",
      "Abstract: Various studies demonstrated that probiotics play important roles in maintaining the balance of microorganisms in the body. Some strains produce bile salt hydrolase enzyme (BSH), which is an indirect mechanism for lowering cholesterol. BSH-producing probiotics as a supplement might be an alternative way to help reducing cholesterol in the body. The aim of this study was to investigate the effects of different microcapsule formulations with selected vegetable powders on growth characteristics of 3 Thai probiotic strains, Lactobacillus gasseri TM1, Lacticaseibacillus rhamnosus TM7, and L. rhamnosus TM14. Probiotics were cultured in MRS broth supplemented with 5 vegetable powders. Corn powder significantly increased growth rate of probiotics from 10^9 to 10^12 CFU/ml. Therefore, different microcapsule formulations by Maillard reaction of whey protein isolate and maltodextrin mixed with and without corn powder were studied. The results showed that probiotic microcapsules formulated with corn powder significantly effectively sustained probiotic viability under gastrointestinal and storage conditions.\n",
      "Text: The encapsulated probiotic cells and probiotic cell suspensions were used in this experiment in order to measure probiotic cells viability before and after freeze drying. The encapsulated probiotic cells were released with PBS, pH 7.2, and then all samples were 10 -fold serially diluted with PBS, pH 7.2 and spread on MRS agar with CaCO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Title: {hit['_source']['metadata']['title']}\")\n",
    "    print(f\"Abstract: {hit['_source']['metadata']['abstract']['p']}\")\n",
    "    print(f\"Text: {hit['_source']['text']}\")\n",
    "    text_sample = hit['_source']['text']\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put this in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a nutritionist specialized in plant-based diets. \n",
    "I will give you some information about myself and you will provide me with good health and diet advice.\n",
    "\n",
    "# ABOUT ME\n",
    "{ABOUT_ME}\n",
    "\n",
    "# CONTEXT\n",
    "{CONTEXT}\n",
    "\n",
    "Please provide concrete advice in less than 250 words, and justify your answer based on the information provided in the context only if it is relevant.\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(example, context):\n",
    "    about_me = example[\"about_me\"] + ' ' + example[\"question\"]\n",
    "    \n",
    "    return PROMPT_TEMPLATE.format(\n",
    "        ABOUT_ME=about_me,\n",
    "        CONTEXT=context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plants have evolved miRNA-target modules to regulate the tolerance to nutrient stress, some of which are evolutionarily related to environmental adaptation. A single miRNA may target more than one transcript, and vice versa, to fine-tune the expression of genes, which converges into a sophisticated and extremely fault-tolerant crosstalk. Accumulating findings highlight that miRNAs furnish a bridge for the transportation and stockpile of N and Pi nutrition in the plants, plant-environment interactions, and plant-plant communications through the modulation of N and Pi signaling transduction.The main themes that emerged from previous studies are the role of miRNAs in enhancing NUE and PUE of plants, as well as the adaptive responses of plants to N and Pi stresses. Destructive effects of nutrient stress on plants are largely dependent on the inheritance and variation of plants and the influence of the environment due to the immobility of plants and the complexity of the environment. One area of future research that needs to be addressed in more detail concerns the function of miRNAs in the communications between plants and the environment. For example, the role of plant symbionts in exporting sRNA to induce transboundary gene silencing to improve N and Pi homeostasis of plants has not been well reported and requires further investigation. In addition, future studies on miRNA*s as underappreciated regulatory components would also expand our understanding of The research directions mainly focus on the upstream regulatory mechanisms and the downstream regulatory roles of miRNAs responding to nutrient stress. In particular, the upstream regulatory mechanisms of miRNAs are still in their infancy, such as the mechanisms by which miRNAs are loaded for precise regulation. For instance, in Arabidopsis, Pi starvation significantly increases the expression of miR778, miR827, and miR2111, and inhibits the expression of miR168, miR395, and miR398. Interestingly, the expression of miR778 and miR2111 quickly decrease by approximately two folds within three hours of the restored Pi supply (Paul et al. Fine-tuning'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = response['hits']['hits'][0]['_source']['text']\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a nutritionist specialized in plant-based diets. \n",
      "I will give you some information about myself and you will provide me with good health and diet advice.\n",
      "\n",
      "# ABOUT ME\n",
      "I am lactose intolerant and want to explore plant-based milk alternatives. What are some plant-based milk options that are lactose-free?\n",
      "\n",
      "# CONTEXT\n",
      "Plants have evolved miRNA-target modules to regulate the tolerance to nutrient stress, some of which are evolutionarily related to environmental adaptation. A single miRNA may target more than one transcript, and vice versa, to fine-tune the expression of genes, which converges into a sophisticated and extremely fault-tolerant crosstalk. Accumulating findings highlight that miRNAs furnish a bridge for the transportation and stockpile of N and Pi nutrition in the plants, plant-environment interactions, and plant-plant communications through the modulation of N and Pi signaling transduction.The main themes that emerged from previous studies are the role of miRNAs in enhancing NUE and PUE of plants, as well as the adaptive responses of plants to N and Pi stresses. Destructive effects of nutrient stress on plants are largely dependent on the inheritance and variation of plants and the influence of the environment due to the immobility of plants and the complexity of the environment. One area of future research that needs to be addressed in more detail concerns the function of miRNAs in the communications between plants and the environment. For example, the role of plant symbionts in exporting sRNA to induce transboundary gene silencing to improve N and Pi homeostasis of plants has not been well reported and requires further investigation. In addition, future studies on miRNA*s as underappreciated regulatory components would also expand our understanding of The research directions mainly focus on the upstream regulatory mechanisms and the downstream regulatory roles of miRNAs responding to nutrient stress. In particular, the upstream regulatory mechanisms of miRNAs are still in their infancy, such as the mechanisms by which miRNAs are loaded for precise regulation. For instance, in Arabidopsis, Pi starvation significantly increases the expression of miR778, miR827, and miR2111, and inhibits the expression of miR168, miR395, and miR398. Interestingly, the expression of miR778 and miR2111 quickly decrease by approximately two folds within three hours of the restored Pi supply (Paul et al. Fine-tuning\n",
      "\n",
      "Please provide concrete advice in less than 250 words, and justify your answer based on the information provided in the context only if it is relevant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_sample = build_prompt(EXAMPLES[15], context)\n",
    "print(prompt_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your lactose intolerance, some plant-based milk alternatives that are lactose-free include almond milk, soy milk, coconut milk, and oat milk. These options are not only lactose-free but also packed with essential nutrients like calcium, vitamin D, and protein to support your overall health.\n",
      "\n",
      "In the context provided, while the information mainly focuses on the role of miRNAs in plant nutrition and stress responses, it highlights the importance of fine-tuning gene expressions to enhance nutrient use efficiency in plants. Similarly, by choosing plant-based milk alternatives, you can optimize your nutrient intake and enhance your overall health by avoiding lactose-containing dairy products that may cause digestive issues for you. Embracing plant-based milk options aligns with the concept of utilizing miRNAs to modulate nutrient uptake and signaling pathways for better nutrient utilization in plants. Therefore, incorporating lactose-free plant-based milk alternatives into your diet can help support your health and well-being in a way that resonates with the principles discussed in the context provided.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "gpt_client = OpenAI(\n",
    "    api_key=open_api_key,\n",
    ")\n",
    "\n",
    "gpt_response = gpt_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_sample}\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "response_sample = gpt_response.choices[0].message.content\n",
    "print(response_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put everything together with functions and we can iterate over examples and save the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(example):\n",
    "    query_text = example['about_me'] + ' ' + example['question']\n",
    "    return query_text\n",
    "\n",
    "def get_query_embedding(query_text, EMBEDDING_MODEL_ID):\n",
    "    embedding_model = get_embedding_model(EMBEDDING_MODEL_ID)\n",
    "    query_embedding = embedding_model.encode(query_text)\n",
    "    return query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query_embedding, size=3):    \n",
    "    search_body = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embedding\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"k\": 5\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"text\", \"metadata\"]\n",
    "    }\n",
    "\n",
    "    response = client.search(index=INDEX_NAME, body=search_body)\n",
    "    \n",
    "    context = \"\"\n",
    "    \n",
    "    for i in range(size):\n",
    "        context += response['hits']['hits'][i]['_source']['text'] + ' '\n",
    "    \n",
    "    return context\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_response(open_api_key, prompt):\n",
    "    gpt_client = OpenAI(api_key=open_api_key)\n",
    "    gpt_response = gpt_client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return gpt_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here we can iterate over examples and save the responses. The we'll want to save everything in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about_me': \"I am obese and just got the weight loss surgery done. I need to increase my protein intake but can't tolerate dairy well.\", 'question': 'What is the most tolerable protein enhancing strategy for someone like me?', 'context': 'Nutritional supplements for sports and exercise (NSSE) refer to products containing carbohydrates, proteins, fats, minerals, vitamins, herbs, enzymes, metabolic intermediates (amino acids), or extracts of various plants/foods [Sports and exercise have become indispensable components of individuals’ lives in contemporary society [Bibliometrics is the interdisciplinary science of quantitative analysis of all knowledge carriers through mathematical and statistical methods [ In the era of competitive sports, an increasing number of people take NSSE [The overall development of research publications in the field of NSSE is favorable and is currently in the stage of a surge in the amount of literature. From a global perspective, North American countries, European countries, and Australia have close cooperation, Brazil contributes significantly to South American countries, and China produces a substantial amount of output in Asian countries. European countries, as the first countries to step into this field, are now at a stage where research attention is not decreasing, followed by South American and Southeast Asian countries as the new research mainstay.Top research organizations worldwide in the field of sports nutrition supplements have conducted in-depth studies in the following thematic directions: (1) The University of Copenhagen, Denmark, focuses on the fact that supplementation with anthocyanin-rich blueberry concentrate improves brain perfusion and activation in brain regions associated with cognitive function in healthy older adults [The top contributing authors and teams conducted in-depth research on the following topics: (1) The Kreider et al. team in the United States focuses on creatine supplementation to enhance post-exercise recovery, injury prevention, thermoregulation, rehabilitation, and concussion or spinal cord neuroprotection in addition to improving sports and exercise [Currently, the most controversial issue in the field is how NSSE should be characterized, in which the attributes of the product are ambiguous between dietary supplements and pharmaceuticals, whether they should be classified as prescription or over-the-counter, how their dosage should be standardized as a baseline, how they should be balanced with the diet after being added to an athlete’s training schedule, and how their efficacy should be related to the indicators in promotional campaigns and their effectiveness in practical use. However, a large part of nutritional supplements can provide athletes with dimensions that cannot be reached in traditional training systems. By analyzing high-frequency keywords, it is clear that the focus of the study group shifted from children in 2000 [In summary, NSSE plays a vital role in athletes’ sports nutrition programs and, if used properly, some supplements can assist athletes in achieving their sports nutrition goals. However, considerable effort and expertise are required to determine which products are appropriate, how to integrate them into an athlete’s sports nutrition program, and how to ensure that any benefits outweigh the possible negative effects. Therefore, when deciding whether to use sports nutritional supplements, athletes and teams should weigh the efficacy against possible risks and controversies. In future research and practice, more scientific evidence and regulatory measures are required to address the current controversies and issues and promote the rational use and development of sports nutrition supplements. This study presents the first extensive overview of sports nutritional supplement research through bibliometric and visual analyses and provides useful insights and analyses on future research directions. Research in the field of the NSSE can be divided into four stages: steady growth (2000–2007), exponential growth (2007–2013), fluctuation (2013–2017) and surge (2017–2024). The United States is the most active country in the field of nutritional supplements. European countries are the early batch of countries in this field, whose attention to research is not decreasing at the current stage, followed by South American and Southeast Asian countries as the emerging research mainstay countries. In recent years, Croatia, Colombia, Slovenia, Chile, Egypt, China, and Thailand have become the dominant countries in this field. The Australian Catholic University is the top research organization in the field. Burke LM from Australia had the most publications. Research in this area has been published in Nutrients in Switzerland. The study population mainly comprised men, and postmenopausal women were the main focus of the female group. Coronary heart and cardiovascular diseases continue to dominate research in this field. A milestone was achieved when the consensus statement was published in large numbers in 2022, signaling the agreement of experts in the field on common issues as well as some specific issues. Insulin resistance, sports nutrition, inflammation, alpha-linolenic acid, limb strength performance, female sex, and gut microbiota are the focus of the current stage of research and future studies. ', 'response': 'Given your recent weight loss surgery and need to increase protein intake without dairy, I recommend incorporating plant-based protein sources into your diet. Some excellent options include tofu, tempeh, edamame, lentils, chickpeas, quinoa, chia seeds, hemp seeds, and nuts. These foods are not only high in protein but also rich in essential nutrients like fiber, vitamins, and minerals.\\n\\nAdditionally, you may consider adding vegan protein powders to your diet, such as pea protein, rice protein, or hemp protein. These powders can easily be mixed into smoothies, oatmeal, or baked goods to boost your protein intake.\\n\\nBy focusing on whole, plant-based foods and incorporating a variety of protein sources into your meals, you can meet your protein needs without relying on dairy products. This approach aligns with the principles of plant-based nutrition and can support your weight loss journey and overall health.\\n\\nBased on the context provided, plant-based diets have been shown to be effective in supporting weight loss and overall health. Plant-based sources of protein are also well tolerated and can help support muscle recovery and maintenance, which is important after weight loss surgery. Additionally, these foods are rich in nutrients that are beneficial for overall well-being.'}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for example in tqdm(EXAMPLES):\n",
    "    query_text = get_query(example)\n",
    "    query_embedding = get_query_embedding(query_text, EMBEDDING_MODEL_ID)\n",
    "    context = get_context(query_embedding)\n",
    "    prompt = build_prompt(example, context)\n",
    "    response = get_gpt_response(open_api_key, prompt)\n",
    "    \n",
    "    full_q_and_a = example.copy()\n",
    "    full_q_and_a['context'] = context\n",
    "    full_q_and_a['response'] = response\n",
    "    print(full_q_and_a)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an example of what it looks like. Next we'll want to save it to a json file, which is done in the generating_training_data.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about_me': \"I am obese and just got the weight loss surgery done. I need to increase my protein intake but can't tolerate dairy well.\",\n",
       " 'question': 'What is the most tolerable protein enhancing strategy for someone like me?',\n",
       " 'context': 'Nutritional supplements for sports and exercise (NSSE) refer to products containing carbohydrates, proteins, fats, minerals, vitamins, herbs, enzymes, metabolic intermediates (amino acids), or extracts of various plants/foods [Sports and exercise have become indispensable components of individuals’ lives in contemporary society [Bibliometrics is the interdisciplinary science of quantitative analysis of all knowledge carriers through mathematical and statistical methods [ In the era of competitive sports, an increasing number of people take NSSE [The overall development of research publications in the field of NSSE is favorable and is currently in the stage of a surge in the amount of literature. From a global perspective, North American countries, European countries, and Australia have close cooperation, Brazil contributes significantly to South American countries, and China produces a substantial amount of output in Asian countries. European countries, as the first countries to step into this field, are now at a stage where research attention is not decreasing, followed by South American and Southeast Asian countries as the new research mainstay.Top research organizations worldwide in the field of sports nutrition supplements have conducted in-depth studies in the following thematic directions: (1) The University of Copenhagen, Denmark, focuses on the fact that supplementation with anthocyanin-rich blueberry concentrate improves brain perfusion and activation in brain regions associated with cognitive function in healthy older adults [The top contributing authors and teams conducted in-depth research on the following topics: (1) The Kreider et al. team in the United States focuses on creatine supplementation to enhance post-exercise recovery, injury prevention, thermoregulation, rehabilitation, and concussion or spinal cord neuroprotection in addition to improving sports and exercise [Currently, the most controversial issue in the field is how NSSE should be characterized, in which the attributes of the product are ambiguous between dietary supplements and pharmaceuticals, whether they should be classified as prescription or over-the-counter, how their dosage should be standardized as a baseline, how they should be balanced with the diet after being added to an athlete’s training schedule, and how their efficacy should be related to the indicators in promotional campaigns and their effectiveness in practical use. However, a large part of nutritional supplements can provide athletes with dimensions that cannot be reached in traditional training systems. By analyzing high-frequency keywords, it is clear that the focus of the study group shifted from children in 2000 [In summary, NSSE plays a vital role in athletes’ sports nutrition programs and, if used properly, some supplements can assist athletes in achieving their sports nutrition goals. However, considerable effort and expertise are required to determine which products are appropriate, how to integrate them into an athlete’s sports nutrition program, and how to ensure that any benefits outweigh the possible negative effects. Therefore, when deciding whether to use sports nutritional supplements, athletes and teams should weigh the efficacy against possible risks and controversies. In future research and practice, more scientific evidence and regulatory measures are required to address the current controversies and issues and promote the rational use and development of sports nutrition supplements. This study presents the first extensive overview of sports nutritional supplement research through bibliometric and visual analyses and provides useful insights and analyses on future research directions. Research in the field of the NSSE can be divided into four stages: steady growth (2000–2007), exponential growth (2007–2013), fluctuation (2013–2017) and surge (2017–2024). The United States is the most active country in the field of nutritional supplements. European countries are the early batch of countries in this field, whose attention to research is not decreasing at the current stage, followed by South American and Southeast Asian countries as the emerging research mainstay countries. In recent years, Croatia, Colombia, Slovenia, Chile, Egypt, China, and Thailand have become the dominant countries in this field. The Australian Catholic University is the top research organization in the field. Burke LM from Australia had the most publications. Research in this area has been published in Nutrients in Switzerland. The study population mainly comprised men, and postmenopausal women were the main focus of the female group. Coronary heart and cardiovascular diseases continue to dominate research in this field. A milestone was achieved when the consensus statement was published in large numbers in 2022, signaling the agreement of experts in the field on common issues as well as some specific issues. Insulin resistance, sports nutrition, inflammation, alpha-linolenic acid, limb strength performance, female sex, and gut microbiota are the focus of the current stage of research and future studies. ',\n",
       " 'response': 'Given your recent weight loss surgery and need to increase protein intake without dairy, I recommend incorporating plant-based protein sources into your diet. Some excellent options include tofu, tempeh, edamame, lentils, chickpeas, quinoa, chia seeds, hemp seeds, and nuts. These foods are not only high in protein but also rich in essential nutrients like fiber, vitamins, and minerals.\\n\\nAdditionally, you may consider adding vegan protein powders to your diet, such as pea protein, rice protein, or hemp protein. These powders can easily be mixed into smoothies, oatmeal, or baked goods to boost your protein intake.\\n\\nBy focusing on whole, plant-based foods and incorporating a variety of protein sources into your meals, you can meet your protein needs without relying on dairy products. This approach aligns with the principles of plant-based nutrition and can support your weight loss journey and overall health.\\n\\nBased on the context provided, plant-based diets have been shown to be effective in supporting weight loss and overall health. Plant-based sources of protein are also well tolerated and can help support muscle recovery and maintenance, which is important after weight loss surgery. Additionally, these foods are rich in nutrients that are beneficial for overall well-being.'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_q_and_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "We are going to fine-tune the Falcon-7B Instruct model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training and Testing Sets\n",
    "\n",
    "First let's split the training dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "training_data_path = Path(\"../modules/q_and_a_dataset/data/training_data.json\")\n",
    "\n",
    "# Load the JSON file\n",
    "with open(training_data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the training data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = Path(\"../modules/model_training/datasets/training_data.json\")\n",
    "test_data_path = Path(\"../modules/model_training/datasets/testing_data.json\")\n",
    "\n",
    "with open(train_data_path, 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(test_data_path, 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Valid Comet API Key saved in C:\\Users\\RaviB\\.comet.config (set COMET_CONFIG to change where it is saved).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "import comet_ml\n",
    "\n",
    "COMET_API_KEY = os.environ.get(\"COMET_API_KEY\")\n",
    "if COMET_API_KEY is None:\n",
    "    raise ValueError(\"COMET_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "comet_ml.login(api_key=COMET_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#model_name = \"openai-community/gpt2\"\n",
    "model_name = \"openai-community/gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "with open(\"C:/Users/RaviB/GitHub/vegan-ai-nutritionist/modules/model_training/datasets/training_data.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"C:/Users/RaviB/GitHub/vegan-ai-nutritionist/modules/model_training/datasets/testing_data.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['about_me', 'question', 'context', 'response'],\n",
       "    num_rows: 60\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Combine 'about_me' and 'context' for the full context\n",
    "    full_context = examples['about_me'] + ' ' + examples['context']\n",
    "    \n",
    "    # Create the prompt using the full context\n",
    "    prompt = f\"Question: {examples['question']}\\nContext: {full_context}\\nAnswer:\"\n",
    "    response = examples['response']\n",
    "    \n",
    "    # Tokenize inputs and labels\n",
    "    tokenized_input = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_output = tokenizer(response, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Create the final input_ids and labels\n",
    "    input_ids = tokenized_input[\"input_ids\"] + tokenized_output[\"input_ids\"][1:]  # Remove BOS token\n",
    "    labels = [-100] * len(tokenized_input[\"input_ids\"]) + tokenized_output[\"input_ids\"][1:]\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1] * len(input_ids), \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Combine 'about_me' and 'context' for the full context\n",
    "    full_context = examples['about_me'] + ' ' + examples['context']\n",
    "    \n",
    "    # Create the prompt using the full context\n",
    "    prompt = f\"Question: {examples['question']}\\nContext: {full_context}\\nAnswer:\"\n",
    "    response = examples['response']\n",
    "    \n",
    "    # Tokenize inputs and labels\n",
    "    tokenized_input = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_output = tokenizer(response, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Combine input and output (GPT-2 is autoregressive)\n",
    "    input_ids = tokenized_input[\"input_ids\"] + tokenized_output[\"input_ids\"]\n",
    "    \n",
    "    # Create the labels (output sequence should be the entire concatenated sequence)\n",
    "    labels = [-100] * len(tokenized_input[\"input_ids\"]) + tokenized_output[\"input_ids\"]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": [1] * len(input_ids), \n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, remove_columns=train_dataset.column_names)\n",
    "tokenized_test = test_dataset.map(tokenize_function, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ravinderrai/ai-vegan-nutritionist/25ae4e6f2cce4efbb324ef171cd7d81b\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda package as an explicit file\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error retrieving Conda information\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(api_key=COMET_API_KEY, project_name=\"ai_vegan_nutritionist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4245584fc55d4918b9c250a7525ab827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  84%|########4 | 1.28G/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3706eb2a935a4814846e9b4dcc2acfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the model without the timeout argument\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                # Directory to save the model and logs\n",
    "    num_train_epochs=1,                    # Number of training epochs\n",
    "    per_device_train_batch_size=2,         # Batch size for training\n",
    "    per_device_eval_batch_size=2,          # Batch size for evaluation\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,                       # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                     # Strength of weight decay\n",
    "    logging_dir=\"./logs\",                  # Directory for storing logs\n",
    "    logging_steps=100,                       # Log every 10 steps\n",
    "    eval_strategy=\"steps\",            # Evaluate every few steps\n",
    "    eval_steps=500,                         # Evaluation frequency\n",
    "    save_steps=1000,                        # Save model every 1000 steps\n",
    "    load_best_model_at_end=True,           # Load the best model at the end of training\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m An experiment with the same configuration options is already running and will be reused.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05ccd54e550419fb8385487845e9c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 21:41:18 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id d3d1502db77a4d358d22649c94fcd100: Failed to log run data: Exception: Changing param values is not allowed. Param with key='torch_dtype' was already logged with value='None' for run ID='d3d1502db77a4d358d22649c94fcd100'. Attempted logging new value 'float32'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 56.4018, 'train_samples_per_second': 1.064, 'train_steps_per_second': 0.124, 'train_loss': 4.175367082868304, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7, training_loss=4.175367082868304, metrics={'train_runtime': 56.4018, 'train_samples_per_second': 1.064, 'train_steps_per_second': 0.124, 'total_flos': 104014477787136.0, 'train_loss': 4.175367082868304, 'epoch': 0.9333333333333333})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the fine-tuned model from the 'results' directory\n",
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_name_or_path = \"./results/checkpoint-7\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about_me': 'I am a sustainability advocate passionate about reducing food waste and am seeking plant-based solutions to minimize environmental impact.',\n",
       " 'question': 'What are some creative ways to use plant-based ingredients to create zero-waste dishes and contribute to sustainable food practices?',\n",
       " 'context': 'Water is often considered an abundant natural resource but only 3% of it is available for consumption with the remaining 97% trapped in the icecaps and snow covers [Waste water irrigation not only minimizes the pressure on freshwater resources, also provide economical support to farmers. Wastewater is an enriched source of organic matter but also contain high amount of various hazardous elements like Cd, Pb, Ni, Cr, Zn, Mn, and Hg etc. which adversely affect the crop yield [In Pakistan, farmers use untreated molasses-based waste water for crop irrigation to avoid the economic pressure of freshwater irrigation, and fertilizers. This poses a significant risk to human and ecosystem health, and these issues are escalating rapidly [Vegetables accumulate toxic elements in edible and non-edible parts more easily as compared to fruits and cereal crops [ BSH activities were observed in The differences in BSH activities of LAB isolates might be due to the influences of disparate environments on expressions of BSH genes (Song et al., Different vegetable powders including cherry tomato, beetroot, pumpkin, bok choy, and corn at 5% (w/v) were added into culture media of The viability of probiotics Moreover, addition of vegetable powders reduced probiotic deaths and stabilized the growth rates of probiotics during the stationary phase (Fig.\\xa0This study was the first study which integrated corn powder into the mixture of microcapsule material. Several studies have reported prebiotic effects of corn compositions. Consumption of SCF for two weeks significantly enhanced bifidobacterial numbers in healthy volunteers (Costabile et al., Before freeze drying, there were approximately 10The survival of probiotic free cells and encapsulated of probiotics before–after freeze drying. WPI and MD have been wildly used as a mixture for microencapsulation because of their Maillard reaction’s property. Corn which is often used as a raw material in many food industries due to its low cost has never been used as encapsulate material. Structure of corn powder is composed of carbohydrates, soluble corn fiber and fibers which are suitable to be used as a wall material. Thus, incorporation of corn powder into the mixture of WPI-MD might strengthen encapsulated surface. It has been reported that chemically modified whey protein and cornstarch formulated into resistant starch (RS) used as an encapsulated material exhibited significantly greater microencapsulation property and protection against environmental microbial destruction than RS alone (Free cells and microencapsulated probiotics were observed under a scanning electron microscopy (SEM) (Fig. The morphology of probiotics microencapsulated. Scanning electron microscopy (SEM) of free cells (It was the first time that this structure was studied and reported. Combination of WPI, MD, and corn formed a reticulum-like structure wrapped in another layer which potentially increasing the encapsulation efficiency (Yan et al., Survivals of microencapsulated probiotics in simulated gastrointestinal conditions were examined using artificial gastric and intestinal fluids. The numbers probiotics free cells were decreased from an initial number of 10Moreover, Table\\xa0The survival of probiotic free cells and encapsulated FormulaNumber of viability (Log CFU/mL)InitialArtificial gastric fluidsArtificial intestinal fluids1 h2 h3 h4 h5 h6 h\\xa0CT9.361\\u2009±\\u20090.059.041\\u2009±\\u20090.1908.602\\u2009±\\u20090.1028.00\\u2009±\\u20090.010000\\xa0M8.014\\u2009±\\u20090.1047.512\\u2009±\\u20090.1027.115\\u2009±\\u20090.156.815\\u2009±\\u20090.1056.360\\u2009±\\u20090.1106.12\\u2009±\\u20090.1076.00\\u2009±\\u20090.05\\xa0MW8.653\\u2009±\\u20090.158.929\\u2009±\\u20090.10510.439\\u2009±\\u20090.4110.243\\u2009±\\u20090.109.929\\u2009±\\u20090.049.778\\u2009±\\u20090.058.301\\u2009±\\u20090.05\\xa0MWC7.30\\u2009±\\u20090.157.525\\u2009±\\u20090.1608.452\\u2009±\\u20090.418.650\\u2009±\\u20090.1010.254\\u2009±\\u20090.0410.00\\u2009±\\u20090.059.58\\u2009±\\u20090.05\\xa0CT9.243\\u2009±\\u20090.058.653\\u2009±\\u20090.1908.602\\u2009±\\u20090.1028.00\\u2009±\\u20090.010000\\xa0M8.519\\u2009±\\u20090.1087.301\\u2009±\\u20090.0107.00\\u2009±\\u20090.0726.284\\u2009±\\u20090.1156.01\\u2009±\\u20090.3205.367\\u2009±\\u20090.0485.106\\u2009±\\u20090.005\\xa0MW9.452\\u2009±\\u20090.3510.20\\u2009±\\u20090.7810.447\\u2009±\\u20090.1010.778\\u2009±\\u20090.259.301\\u2009±\\u20090.1059.01\\u2009±\\u20090.0488.512\\u2009±\\u20090.110\\xa0MWC6.550\\u2009±\\u20090.3057.00\\u2009±\\u20090.1707.52\\u2009±\\u20090.1057.98\\u2009±\\u20090.2510.00\\u2009±\\u20090.4559.820\\u2009±\\u20090.4089.05\\u2009±\\u20090.20\\xa0CT9.278\\u2009±\\u20090.0678.954\\u2009±\\u20090.1028.176\\u2009±\\u20090.307.698\\u2009±\\u20090.145000\\xa0M8.140\\u2009±\\u20090.6007.845\\u2009±\\u20090.1257.501\\u2009±\\u20090.016.450\\u2009±\\u20090.2306.205\\u2009±\\u20090.1106.07\\u2009±\\u20090.4055.95\\u2009±\\u20090.085\\xa0MW9.20\\u2009±\\u20090.1089.929\\u2009±\\u20090.10810.74\\u2009±\\u20090.03510.45\\u2009±\\u20090.06510.04\\u2009±\\u20090.5059.653\\u2009±\\u20090.1409.107\\u2009±\\u20090.750\\xa0MWC6.507\\u2009±\\u20090.1087.000\\u2009±\\u20090.1087.250\\u2009±\\u20090.0357.860\\u2009±\\u20090.06510.658\\u2009±\\u20090.50510.754\\u2009±\\u20090.14010.25\\u2009±\\u20090.750Data are represented as means\\u2009±\\u2009SD (n\\u2009=\\u20095)Viabilities of free cells (CT) and microencapsulated probiotics stored at 4\\xa0°C and 25\\xa0°C (room temperature) for 1 year are shown in Fig.\\xa0Stability of probiotics encapsulated and free cells during long-term storage at 4\\xa0°C and 25\\xa0°C. M was MD alone. MW was the mixture of MD-WPI. MWC was the mixture of MD-WPI, and CT was free cells used as a control. Data are represented as means\\u2009±\\u2009SD means (n\\u2009=\\u20095). ****Bile salt hydrolase enzyme activities were maintained in microencapsulated probiotics. These probiotics continued to produce BSH with the same potency as probiotics without microencapsulation as shown in Table\\xa0 Bile salt hydrolase (BSH) enzyme activity of probiotics under each experimental conditionConditionBSH activityCTMMWMWCCTMMWMWCCTMMWMWCBefore freeze drying\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009After freeze drying\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009\\u2009+\\u2009Artificial gastric Present study evaluated the impacts of molasses-based waste water irrigation on carrot growth, yield and nutritional quality. Results revealed that the effects of untreated molasses-based waste water exhibited significant reduction in growth, yield, physiology, metabolism, and nutritional contents of carrot. Additionally, Cd and Pb contents accumulation in carrot root of plants irrigated with untreated molasses-based waste water were found to exceed the permissible limits suggested by WHO and their consumption will cause health risks. While, diluted molasses-based waste water irrigation positively enhanced the growth, yield of carrot plants without affecting the nutritional quality. This strategy is cost effective, appeared as most appropriate alternative mean to reduce the freshwater consumption in water deficit regions of the world. This study suggest that policymakers should focus to develop guidelines for safe use of waste water irrigation and implement by raising awareness in farmers regarding risk associated with untreated waste water irrigation. The future research must be focused on developing strategies such as heavy metals remediation to avoid their accumulation in crops, and long term impact of diluted waste water irrigations on the soil health as well.',\n",
       " 'response': 'As a sustainability advocate seeking plant-based solutions, one creative way to minimize environmental impact and contribute to sustainable food practices is by incorporating a variety of vegetable powders into your dishes. Vegetable powders, such as cherry tomato, beetroot, pumpkin, bok choy, and corn, can not only enhance the flavor and nutritional content of your meals but also help reduce food waste by utilizing the entire vegetable, including parts that are often discarded. \\n\\nIncorporating vegetable powders into your diet can also have health benefits, as they are rich in essential nutrients and antioxidants. Additionally, vegetable powders have been shown to support the growth of probiotics in the gut, which can improve digestion and overall gut health. \\n\\nConsidering the information provided in the context about water scarcity and the use of wastewater for irrigation, it is important to be mindful of the sources of your ingredients. Opt for organic and locally sourced vegetable powders to ensure that they are free from hazardous elements that may affect crop yield and human health. By choosing sustainable ingredients and incorporating vegetable powders into your meals, you can not only reduce food waste but also support your health and the environment.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_example, tokenizer):\n",
    "    # Combine 'about_me' and 'context' for the full context\n",
    "    full_context = test_example['about_me'] + ' ' + test_example['context']\n",
    "    \n",
    "    # Create the prompt using the full context (as done during training)\n",
    "    prompt = f\"Question: {test_example['question']}\\nContext: {full_context}\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the input prompt\n",
    "    tokenized_input = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "    \n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_datapoint = preprocess_test_data(test_data[1], tokenizer)\n",
    "\n",
    "sample_datapoint = {\n",
    "    \"input_ids\": sample_datapoint[\"input_ids\"].to(device),\n",
    "    \"attention_mask\": sample_datapoint[\"attention_mask\"].to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: Question: What are some creative ways to use plant-based ingredients to create zero-waste dishes and contribute to sustainable food practices?\n",
      "Context: I am a sustainability advocate passionate about reducing food waste and am seeking plant-based solutions to minimize environmental impact. Water is often considered an abundant natural resource but only 3% of it is available for consumption with the remaining 97% trapped in the icecaps and snow covers [Waste water irrigation not only minimizes the pressure on freshwater resources, also provide a source of fresh water for local communities] [1].\n",
      "I am currently working on a project to reduce the amount of water used to irrigate plants in my garden. I have been experimenting with different ways of using water to achieve this goal. One of the most effective ways I found was to grow my own herbs and vegetables in a pot and then use the water from the pot to water my plants [2], [3] – [5] (see also [6] for more information on how to do this).\n",
      "In this post, I would like to share with you some of my favorite recipes that I've found to be very effective in reducing water use and improving the quality of life for my family and friends.\n",
      "1. Vegetable Soup\n",
      "This soup is a great way to start your day. It is rich in protein, fiber, vitamins, minerals, antioxidants, and is low in fat and calories. You can also use it as an appetizer or as a main dish for a dinner party!\n",
      "Ingredients:\n",
      "2 cups vegetable broth\n",
      "3 cups water\n",
      "Directions: In a large pot, add the broth and water and bring to a boil. Reduce the heat to medium-low and let simmer for 10-15 minutes. Remove from heat and allow to cool to room temperature. Add the salt and pepper to taste and stir to combine. Place the soup into a blender or food processor and blend until smooth and creamy. Store in an airtight container for up to 3 days. Recipe Notes: This soup can be made ahead of time and reheated for an additional 5-10 minutes before serving. Nutrition Information Yield: 4 servings, Serving Size: 1/4 cup Amount Per Serving:\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        sample_datapoint[\"input_ids\"],\n",
    "        attention_mask=sample_datapoint[\"attention_mask\"],\n",
    "        max_length=512,\n",
    "        #max_new_tokens=170,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print the generated response\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Generated response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Answer:\" in generated_text:\n",
    "    generated_text = generated_text.split(\"Answer:\")[1].strip()  # Keep only the answer part\n",
    "    print(f\"Generated response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLora\n",
    "\n",
    "Here we try to use a qlora version of falcon as it requires less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\falconinstruct\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "#from comet_ml import API\n",
    "from peft import LoraConfig, PeftConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "CACHE_DIR = Path.home() / \".cache\" / \"vegan-llms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88644b9183c84f0c9837079861d6c257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\huggingface_hub-0.25.0-py3.8.egg\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RaviB\\.cache\\huggingface\\hub\\models--tiiuae--falcon-7b-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282635a576564596845728a05b09434e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e705b831ef74498aa0ff73b0b79a83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiiuae/falcon-7b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#load_in_4bit=True,\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#cache_dir=str(CACHE_DIR) if CACHE_DIR else None,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\transformers\\modeling_utils.py:3769\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3766\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   3767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m   3768\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 3769\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3778\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3780\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3781\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3785\u001b[0m     is_safetensors_available()\n\u001b[0;32m   3786\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   3787\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3788\u001b[0m ):\n\u001b[0;32m   3789\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\transformers\\utils\\hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\huggingface_hub-0.25.0-py3.8.egg\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\huggingface_hub-0.25.0-py3.8.egg\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\huggingface_hub-0.25.0-py3.8.egg\\huggingface_hub\\file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1230\u001b[0m     )\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\huggingface_hub-0.25.0-py3.8.egg\\huggingface_hub\\file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1381\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\huggingface_hub-0.25.0-py3.8.egg\\huggingface_hub\\file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1915\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\huggingface_hub-0.25.0-py3.8.egg\\huggingface_hub\\file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    539\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    543\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    "    revision=\"main\",\n",
    "    quantization_config=bnb_config,\n",
    "    #load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    #cache_dir=str(CACHE_DIR) if CACHE_DIR else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    "    trust_remote_code=False,\n",
    "    truncation=True,\n",
    "    #cache_dir=str(cache_dir) if cache_dir else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "    with torch.no_grad():\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"query_key_value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_checkpointing = True\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.config.use_cache = False  # Disable caching for gradient checkpointing\n",
    "else:\n",
    "    model.gradient_checkpointing_disable()\n",
    "    model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_perplexity(predictions: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute perplexity metric.\n",
    "\n",
    "    Parameters:\n",
    "    predictions (np.ndarray): Array of predicted values.\n",
    "\n",
    "    Returns:\n",
    "    float: Perplexity metric value.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.exp(predictions.mean()).item()\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(self, eval_pred: EvalPrediction):\n",
    "    \"\"\"\n",
    "    Computes the perplexity metric.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (EvalPrediction): The evaluation prediction.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the perplexity metric.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"perplexity\": compute_perplexity(eval_pred.predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/RaviB/GitHub/vegan-ai-nutritionist'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915cce1544cc4161a2087da4d467ae3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files={'train': 'modules/model_training/datasets/training_data.json'})['train']\n",
    "validation_dataset = load_dataset('json', data_files={'validation': 'modules/model_training/datasets/testing_data.json'})['validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9b8bcaac83441a80f0471193ba1149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Combine 'about_me' and 'context' for the full context\n",
    "    full_context = examples['about_me'] + ' ' + examples['context']\n",
    "    \n",
    "    # Create the prompt using the full context\n",
    "    prompt = f\"Question: {examples['question']}\\nContext: {full_context}\\nAnswer:\"\n",
    "    response = examples['response']\n",
    "    \n",
    "    # Tokenize inputs and labels\n",
    "    tokenized_input = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_output = tokenizer(response, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Create the final input_ids and labels\n",
    "    input_ids = tokenized_input[\"input_ids\"] + tokenized_output[\"input_ids\"][1:]  # Remove BOS token\n",
    "    labels = [-100] * len(tokenized_input[\"input_ids\"]) + tokenized_output[\"input_ids\"][1:]\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": [1] * len(input_ids), \"labels\": labels}\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, remove_columns=train_dataset.column_names)\n",
    "tokenized_test = validation_dataset.map(tokenize_function, remove_columns=validation_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b254faba84245bcbf9cda3837205eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72791d205af4f219e1822d117993278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_data(example):\n",
    "    # Combine 'question' and 'context' to form a 'prompt'\n",
    "    prompt = f\"Question: {example['question']}\\nContext: {example['context']}\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    tokenized = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    \n",
    "    # Return the 'prompt' field along with tokenized input\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# Apply the function to your dataset\n",
    "tokenized_train_dataset = train_dataset.map(prepare_data, remove_columns=train_dataset.column_names)\n",
    "tokenized_val_dataset = validation_dataset.map(prepare_data, remove_columns=validation_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "assert isinstance(peft_model, PeftModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravib/vegan_llm/vegan_llm/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ravib/vegan_llm/vegan_llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ravib/vegan_llm/vegan_llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ravib/vegan_llm/vegan_llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[0;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_val_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#peft_config=lora_config,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/vegan_llm/vegan_llm/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vegan_llm/vegan_llm/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:401\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    396\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m     )\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/vegan_llm/vegan_llm/lib/python3.12/site-packages/transformers/trainer.py:501\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# At this stage the model is already loaded\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_peft_model(model):\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for more details\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantization_method_supports_training:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model you are trying to fine-tune is quantized with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mhf_quantizer\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to request the support for training support for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mhf_quantizer\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker\n",
    "\n",
    "Now we are going to set up the fine-tuning process using a sagemaker training job to make use of their GPU instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading Data to S3\n",
    "\n",
    "First we need to upload the data to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to s3://fine-tuning-training-data/train_data.json\n",
      "Testing data saved to s3://fine-tuning-training-data/test_data.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "train_data_path = Path(\"../modules/model_training/datasets/training_data.json\")\n",
    "test_data_path = Path(\"../modules/model_training/datasets/testing_data.json\")\n",
    "\n",
    "with open(train_data_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(test_data_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = \"fine-tuning-training-data\"\n",
    "\n",
    "train_data_key = \"train_data.json\"  # S3 key for training data\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=train_data_key,\n",
    "    Body=json.dumps(train_data)  # Convert the training data to JSON string\n",
    ")\n",
    "\n",
    "test_data_key = \"test_data.json\"  # S3 key for testing data\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=test_data_key,\n",
    "    Body=json.dumps(test_data)  # Convert the testing data to JSON string\n",
    ")\n",
    "\n",
    "print(f\"Training data saved to s3://{bucket_name}/{train_data_key}\")\n",
    "print(f\"Testing data saved to s3://{bucket_name}/{test_data_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test access to saved data in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Set up the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = \"fine-tuning-training-data\"\n",
    "\n",
    "# Define the keys for the training and testing data\n",
    "train_data_key = \"train_data.json\"\n",
    "test_data_key = \"test_data.json\"\n",
    "\n",
    "# Function to load data from S3\n",
    "def load_data_from_s3(bucket, key):\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    data = response['Body'].read().decode('utf-8')  # Read the body and decode it\n",
    "    return json.loads(data)  # Load the JSON data\n",
    "\n",
    "# Load training and testing data\n",
    "train_data = load_data_from_s3(bucket_name, train_data_key)\n",
    "test_data = load_data_from_s3(bucket_name, test_data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample Data: {'about_me': 'I am a new mother considering a plant-based diet for my baby.', 'question': 'Is a plant-based diet safe and nutritious for infants?', 'context': 'Statistical analysis exhibited that effect of irrigation treatments was non-significant on crude protein, crude fat, crude carbohydrate, crude fiber, and total ash contents of carrot juice except moisture content (Fig.\\xa0Classified filled violin plots Mean comparison analysis for proximate composition, vitamins, and minerals in carrot roots treated with different irrigation treatmentsTreatmentsMoisture(%)Crude protein(%)Crude fat(%)Carbohydrates(%)Crude fiber(%)Total ash(%)T88.8\\u2009±\\u20091.7 a0.9\\u2009±\\u20090.23 a0.2\\u2009±\\u20090.11 a10.6\\u2009±\\u20091.21 a2.4\\u2009±\\u20090.2 a2.3\\u2009±\\u20090.21 aT84\\u2009±\\u20091.65 b0.7\\u2009±\\u20090.13 a0.2\\u2009±\\u20090.09 a8.6\\u2009±\\u20091.35 ab1.2\\u2009±\\u20090.25 b1.1\\u2009±\\u20090.16 bT83\\u2009±\\u20091.45 b0.5\\u2009±\\u20090.03 a0.7\\u2009±\\u20090.04 a5.2\\u2009±\\u20090.27 b1.2\\u2009±\\u20090.11 b1.2\\u2009±\\u20090.06 bT5.3\\u2009±\\u20091.23 a0.04\\u2009±\\u20090.01 a0.02\\u2009±\\u20090.0 a0.2\\u2009±\\u20090.1 a4.2\\u2009±\\u20090.23 a89.8\\u2009±\\u20091.7 aT3.1\\u2009±\\u20091.42 b0.002\\u2009±\\u20090.01b0.008\\u2009±\\u20090.0 b0.03\\u2009±\\u20090.0 b2.4\\u2009±\\u20091.34 b75.1\\u2009±\\u20091.65 bT2.56\\u2009±\\u20090.98 c0.002\\u2009±\\u20090.00 b0.003\\u2009±\\u20090.1 b0.002\\u2009±\\u20090.0 b2.1\\u2009±\\u20091.45 b67.3\\u2009±\\u20091.45 bT34\\u2009±\\u20091.7 a40\\u2009±\\u20090.21a25\\u2009±\\u20090.14a9.2\\u2009±\\u20091.0a42.2\\u2009±\\u20091.2 aT27\\u2009±\\u20090.4 b32\\u2009±\\u20090.13b16\\u2009±\\u20091.23b4.4\\u2009±\\u20090.21b35.1\\u2009±\\u20090.3 bT22\\u2009±\\u20090.32c27\\u2009±\\u20090.15c12\\u2009±\\u20091.47c3.21\\u2009±\\u20090.27b26.1\\u2009±\\u20090.12 cT0.001\\u2009±\\u20090.0 b0.001\\u2009±\\u20090.0 b0.001\\u2009±\\u20090.0 b0.001\\u2009±\\u20090.0 b0.001\\u2009±\\u20090.0 bT0.04\\u2009±\\u20090.12 a0.87\\u2009±\\u20090.11 a1.53\\u2009±\\u20090.76 a0.014\\u2009±\\u20090.12 a0.005\\u2009±\\u20091.22 abT0.078\\u2009±\\u20090.25 a0.93\\u2009±\\u20090.12 a1.69\\u2009±\\u20090.34 a2.96\\u2009±\\u20090.03 a2.41\\u2009±\\u20090.23 a The results indicated that all growth traits except seed germination (that showed no correlation) and yield traits, biochemical contents in leaves except non-reducing sugars (that is negatively correlated with almost all traits) and physical parameters of juice except pH of juice (negatively correlated), total soluble sugars and crude fibers are positively correlated with their own and each other’s traits. However, All minerals and vitamin content and almost all biochemical contents in carrot juice showed negative correlation with most of the parameters except biochemical, mineral and vitamins contents in carrot juice showed positive correlation with pH of the juice, dietary fibers, vitamin A, moisture contents, crude proteins, crude carbohydrates, crud fibers, total ash contents, Ca, Na, P, Mg and K, similarly, metal and vitamin contents in carrot juice showed positive correlation with their own contents and metal contents with total soluble solids and crude fats (Fig.\\xa0The PCA showed the distribution of irrigation treatments on carrot growth, metabolism, nutrition quality and yield (Fig.\\xa0Pearson’s correlation coefficients among 49 pairs of response variables of carrot cultivar treated with different irrigation treatmentsBiplot of principal components analysis of 3 irrigation treatments and 49 response variables of carrot cultivar T-29Cluster based heatmap of three irrigation treatments and 49 response variables of carrot cultivar (T-29) Different vegetable powders including cherry tomato, beetroot, pumpkin, bok choy, and corn at 5% (w/v) were added into culture media of The viability of probiotics Moreover, addition of vegetable powders reduced probiotic deaths and stabilized the growth rates of probiotics during the stationary phase (Fig.\\xa0This study was the first study which integrated corn powder into the mixture of microcapsule material. Several studies have reported prebiotic effects of corn compositions. Consumption of SCF for two weeks significantly enhanced bifidobacterial numbers in healthy volunteers (Costabile et al.,', 'response': \"Congratulations on considering a plant-based diet for your baby! Plant-based diets can be safe and nutritious for infants if properly planned. Make sure to include a variety of nutrient-dense plant foods such as fruits, vegetables, whole grains, legumes, nuts, and seeds to ensure your baby is getting all the essential nutrients. \\n\\nBased on the context provided, it is important to note that carrots are a great source of vitamins, minerals, and fiber. You can incorporate pureed or mashed carrots into your baby's diet to provide them with essential nutrients. Additionally, incorporating other plant foods like cherry tomatoes, beetroots, pumpkins, bok choy, and corn can add variety and additional nutrients to your baby's diet. \\n\\nWhen introducing solid foods to your baby, start with single-ingredient purees and gradually introduce new foods while closely monitoring for any allergies or intolerances. It's also important to consult with a pediatrician or a registered dietitian specializing in plant-based infant nutrition to ensure your baby is meeting their nutritional needs. \\n\\nOverall, with careful planning and consideration, a plant-based diet can provide your baby with all the necessary nutrients for healthy growth and development.\"}\n",
      "Testing Sample Data: {'about_me': 'I have a family history of heart disease and want to improve my heart health.', 'question': 'What plant-based diet is recommended for improving heart health?', 'context': 'This study presents the first extensive overview of sports nutritional supplement research through bibliometric and visual analyses and provides useful insights and analyses on future research directions. Research in the field of the NSSE can be divided into four stages: steady growth (2000–2007), exponential growth (2007–2013), fluctuation (2013–2017) and surge (2017–2024). The United States is the most active country in the field of nutritional supplements. European countries are the early batch of countries in this field, whose attention to research is not decreasing at the current stage, followed by South American and Southeast Asian countries as the emerging research mainstay countries. In recent years, Croatia, Colombia, Slovenia, Chile, Egypt, China, and Thailand have become the dominant countries in this field. The Australian Catholic University is the top research organization in the field. Burke LM from Australia had the most publications. Research in this area has been published in Nutrients in Switzerland. The study population mainly comprised men, and postmenopausal women were the main focus of the female group. Coronary heart and cardiovascular diseases continue to dominate research in this field. A milestone was achieved when the consensus statement was published in large numbers in 2022, signaling the agreement of experts in the field on common issues as well as some specific issues. Insulin resistance, sports nutrition, inflammation, alpha-linolenic acid, limb strength performance, female sex, and gut microbiota are the focus of the current stage of research and future studies. For all analyses performed in this work, we used full-bloom flowers (Fig.\\xa0Corolla and calyxes of the three The seeds of The function of miRNAs in maintaining N and Pi homeostasis can be explored by fine-tuning the Widely-used methods and technologies for accelerating plant breeding via fine-tuning Currently, the physiological and molecular verification and application of transgenic plants represents the most critical step for the implementation of basic research and the advancement of sustainable agriculture. Thus, we focus on discussing the technologies for fine-tuning miRNAs in transgenic research in the following (Fig.\\xa0Straightforward technologies of manipulating miRNA expression are the overexpression of Meanwhile, the clustered regularly interspaced short palindromic repeats CRISPR and CRISPR/Cas system modifying DNA sequences have revolutionized biotechnology and provided genetic tools for genetically modified plants (Manghwar et al. So far, there are limitations and opportunities in the application of CRISPR/Cas system in miRNA editing breeding. The knockout is mainly based on the canonical or discontinued SpCas9 that can induce Efficient', 'response': 'To improve heart health, I recommend following a plant-based diet rich in fruits, vegetables, whole grains, legumes, nuts, and seeds. These foods are high in fiber, antioxidants, and essential nutrients that can help lower cholesterol levels, reduce inflammation, and support overall heart health. Include sources of omega-3 fatty acids such as flaxseeds, chia seeds, and walnuts to further support heart health.\\n\\nBased on the information provided in the context, it is important to focus on cardiovascular diseases in the plant-based diet recommendation, as research in the field of nutritional supplements shows that these diseases continue to dominate. By following a plant-based diet, you can reduce the risk of heart disease and improve your heart health, as this type of diet has been shown to lower the risk of developing cardiovascular diseases. Additionally, the focus on inflammation in the research aligns with the anti-inflammatory properties of plant-based foods, further supporting the recommendation for a plant-based diet to improve heart health.'}\n"
     ]
    }
   ],
   "source": [
    "# Print the loaded data to verify\n",
    "print(\"Training Sample Data:\", train_data[0])\n",
    "print(\"Testing Sample Data:\", test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Sagemaker Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\RaviB\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "sagemaker_role = os.environ.get(\"SAGEMAKER_ROLE\")\n",
    "huggingface_access_token = os.environ.get(\"HUGGINGFACE_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"fine-tuning-training-data\"\n",
    "train_data_s3_uri = f\"s3://{bucket_name}/train_data.json\"\n",
    "test_data_s3_uri = f\"s3://{bucket_name}/test_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model_name_or_path\": \"tiiuae/falcon-7b-instruct\",\n",
    "    \"train_file\": train_data_s3_uri,\n",
    "    \"validation_file\": test_data_s3_uri,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"fp16\": True,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"save_steps\": 1000,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    \"output_dir\": \"s3://falcon-artifact/\", # default value is \"/opt/ml/model\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../modules/model_training/src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../modules/model_training/src/train.py\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Combine 'about_me' and 'context' for the full context\n",
    "    full_context = examples['about_me'] + ' ' + examples['context']\n",
    "    \n",
    "    # Create the prompt using the full context\n",
    "    prompt = f\"Question: {examples['question']}\\nContext: {full_context}\\nAnswer:\"\n",
    "    response = examples['response']\n",
    "    \n",
    "    # Tokenize inputs and labels\n",
    "    tokenized_input = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_output = tokenizer(response, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Combine input and output (GPT-2 is autoregressive)\n",
    "    input_ids = tokenized_input[\"input_ids\"] + tokenized_output[\"input_ids\"]\n",
    "    \n",
    "    # Create the labels (output sequence should be the entire concatenated sequence)\n",
    "    labels = [-100] * len(tokenized_input[\"input_ids\"]) + tokenized_output[\"input_ids\"]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": [1] * len(input_ids), \n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset('json', data_files={\"train\": \"/opt/ml/input/data/train/train_data.json\"})['train']\n",
    "test_dataset = load_dataset('json', data_files={\"test\": \"/opt/ml/input/data/test/test_data.json\"})['test']\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, remove_columns=train_dataset.column_names)\n",
    "tokenized_test = test_dataset.map(tokenize_function, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"s3://falcon-artifact/\", # default value is \"/opt/ml/model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    logging_dir=\"/opt/ml/logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need at least 16 GB of memory on the GPU to run falcon 7B instruct. You can find the options here but ml.p3.2xlarge should work.\n",
    "\n",
    "https://aws.amazon.com/sagemaker/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",  # Python script to launch training\n",
    "    source_dir=\"../modules/model_training/src\",  # Folder where your train script and requirements.txt are\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_count=1,\n",
    "    role=sagemaker_role,\n",
    "    transformers_version=\"4.11\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    py_version=\"py38\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    environment = {\"HUGGINGFACEHUB_TOKEN\": huggingface_access_token}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-10-13-15-15-50-943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:15:53 Starting - Starting the training job\n",
      "2024-10-13 15:15:53 Pending - Training job waiting for capacity......\n",
      "2024-10-13 15:16:34 Pending - Preparing the instances for training...\n",
      "2024-10-13 15:17:23 Downloading - Downloading input data...\n",
      "2024-10-13 15:17:43 Downloading - Downloading the training image........................\n",
      "2024-10-13 15:22:11 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-10-13 15:22:24,533 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-10-13 15:22:24,560 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-10-13 15:22:24,563 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-10-13 15:22:24,820 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_train\": true,\n",
      "        \"eval_steps\": 500,\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"fp16\": true,\n",
      "        \"logging_steps\": 100,\n",
      "        \"model_name_or_path\": \"tiiuae/falcon-7b-instruct\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"s3://falcon-artifact/\",\n",
      "        \"per_device_eval_batch_size\": 1,\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"save_steps\": 1000,\n",
      "        \"train_file\": \"s3://fine-tuning-training-data/train_data.json\",\n",
      "        \"validation_file\": \"s3://fine-tuning-training-data/test_data.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-10-13-15-15-50-943\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-590184030535/huggingface-pytorch-training-2024-10-13-15-15-50-943/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"do_eval\":true,\"do_train\":true,\"eval_steps\":500,\"evaluation_strategy\":\"steps\",\"fp16\":true,\"logging_steps\":100,\"model_name_or_path\":\"tiiuae/falcon-7b-instruct\",\"num_train_epochs\":1,\"output_dir\":\"s3://falcon-artifact/\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"save_steps\":1000,\"train_file\":\"s3://fine-tuning-training-data/train_data.json\",\"validation_file\":\"s3://fine-tuning-training-data/test_data.json\"}\n",
      "SM_USER_ENTRY_POINT=train.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-590184030535/huggingface-pytorch-training-2024-10-13-15-15-50-943/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"eval_steps\":500,\"evaluation_strategy\":\"steps\",\"fp16\":true,\"logging_steps\":100,\"model_name_or_path\":\"tiiuae/falcon-7b-instruct\",\"num_train_epochs\":1,\"output_dir\":\"s3://falcon-artifact/\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"save_steps\":1000,\"train_file\":\"s3://fine-tuning-training-data/train_data.json\",\"validation_file\":\"s3://fine-tuning-training-data/test_data.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2024-10-13-15-15-50-943\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-590184030535/huggingface-pytorch-training-2024-10-13-15-15-50-943/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "SM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--eval_steps\",\"500\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"True\",\"--logging_steps\",\"100\",\"--model_name_or_path\",\"tiiuae/falcon-7b-instruct\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"s3://falcon-artifact/\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"1\",\"--save_steps\",\"1000\",\"--train_file\",\"s3://fine-tuning-training-data/train_data.json\",\"--validation_file\",\"s3://fine-tuning-training-data/test_data.json\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_DO_EVAL=true\n",
      "SM_HP_DO_TRAIN=true\n",
      "SM_HP_EVAL_STEPS=500\n",
      "SM_HP_EVALUATION_STRATEGY=steps\n",
      "SM_HP_FP16=true\n",
      "SM_HP_LOGGING_STEPS=100\n",
      "SM_HP_MODEL_NAME_OR_PATH=tiiuae/falcon-7b-instruct\n",
      "SM_HP_NUM_TRAIN_EPOCHS=1\n",
      "SM_HP_OUTPUT_DIR=s3://falcon-artifact/\n",
      "SM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\n",
      "SM_HP_SAVE_STEPS=1000\n",
      "SM_HP_TRAIN_FILE=s3://fine-tuning-training-data/train_data.json\n",
      "SM_HP_VALIDATION_FILE=s3://fine-tuning-training-data/test_data.json\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.8 train.py --do_eval True --do_train True --eval_steps 500 --evaluation_strategy steps --fp16 True --logging_steps 100 --model_name_or_path tiiuae/falcon-7b-instruct --num_train_epochs 1 --output_dir s3://falcon-artifact/ --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --save_steps 1000 --train_file s3://fine-tuning-training-data/train_data.json --validation_file s3://fine-tuning-training-data/test_data.json\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5332e93b08ad0c47/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50...\n",
      "#015Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 41.9kB/s]\n",
      "#015Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 665/665 [00:00<00:00, 995kB/s]\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "#015Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]#015Downloading: 100%|██████████| 0.99M/0.99M [00:00<00:00, 39.5MB/s]\n",
      "#015Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 446k/446k [00:00<00:00, 33.5MB/s]\n",
      "#015Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]#015Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 27.8MB/s]\n",
      "Using custom data configuration default-5332e93b08ad0c47\n",
      "#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 7281.78it/s]\n",
      "#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 1236.53it/s]\n",
      "#0150 tables [00:00, ? tables/s]Failed to read file '/opt/ml/input/data/train/train_data.json' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column() changed from object to array in row 0\n",
      "#015                            #015Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py\", line 118, in _generate_tables\n",
      "    pa_table = paj.read_json(\n",
      "  File \"pyarrow/_json.pyx\", line 246, in pyarrow._json.read_json\n",
      "  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to array in row 0\n",
      "During handling of the above exception, another exception occurred:\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 37, in <module>\n",
      "    train_dataset = load_dataset('json', data_files={\"train\": \"/opt/ml/input/data/train/train_data.json\"})['train']\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\n",
      "    builder_instance.download_and_prepare(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 1185, in _prepare_split\n",
      "    for key, table in utils.tqdm(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/std.py\", line 1180, in __iter__\n",
      "2024-10-13 15:22:28,040 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "Command \"/opt/conda/bin/python3.8 train.py --do_eval True --do_train True --eval_steps 500 --evaluation_strategy steps --fp16 True --logging_steps 100 --model_name_or_path tiiuae/falcon-7b-instruct --num_train_epochs 1 --output_dir s3://falcon-artifact/ --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --save_steps 1000 --train_file s3://fine-tuning-training-data/train_data.json --validation_file s3://fine-tuning-training-data/test_data.json\"\n",
      "#015Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 26.0/26.0 [00:00<00:00, 41.9kB/s]\n",
      "#015Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 665/665 [00:00<00:00, 995kB/s]\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py\", line 140, in _generate_tables\n",
      "#015Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 0.99M/0.99M [00:00<00:00, 39.5MB/s]\n",
      "#015Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 446k/446k [00:00<00:00, 33.5MB/s]\n",
      "#015Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 1.29M/1.29M [00:00<00:00, 27.8MB/s]\n",
      "    f\"Not able to read records in the JSON file at {file}. \"\n",
      "AttributeError: 'list' object has no attribute 'keys'\n",
      "Using custom data configuration default-5332e93b08ad0c47\n",
      "#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|ââââââââââ| 1/1 [00:00<00:00, 7281.78it/s]\n",
      "#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|ââââââââââ| 1/1 [00:00<00:00, 1236.53it/s]\n",
      "#0150 tables [00:00, ? tables/s]Failed to read file '/opt/ml/input/data/train/train_data.json' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column() changed from object to array in row 0\n",
      "#015                            #015Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py\", line 118, in _generate_tables\n",
      "    pa_table = paj.read_json(\n",
      "  File \"pyarrow/_json.pyx\", line 246, in pyarrow._json.read_json\n",
      "  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to array in row 0\n",
      "During handling of the above exception, another exception occurred:\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 37, in <module>\n",
      "    train_dataset = load_dataset('json', data_files={\"train\": \"/opt/ml/input/data/train/train_data.json\"})['train']\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/load.py\", line 1112, in load_dataset\n",
      "    builder_instance.download_and_prepare(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 636, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/builder.py\", line 1185, in _prepare_split\n",
      "    for key, table in utils.tqdm(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/std.py\", line 1180, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py\", line 140, in _generate_tables\n",
      "    f\"Not able to read records in the JSON file at {file}. \"\n",
      "AttributeError: 'list' object has no attribute 'keys'\n",
      "\n",
      "2024-10-13 15:22:40 Uploading - Uploading generated training model\n",
      "2024-10-13 15:22:40 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2024-10-13-15-15-50-943: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.8 train.py --do_eval True --do_train True --eval_steps 500 --evaluation_strategy steps --fp16 True --logging_steps 100 --model_name_or_path tiiuae/falcon-7b-instruct --num_train_epochs 1 --output_dir s3://falcon-artifact/ --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --save_steps 1000 --train_file s3://fine-tuning-training-data/train_data.json --validation_file s3://fine-tuning-training-data/test_data.json\"\n\rDownloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 26.0/26.0 [00:00<00:00, 41.9kB/s]\n\rDownloading:   0%|          | 0.00/665 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 665/665 [00:00<00:00, 995kB/s]\n/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transform, exit code: 1. Check troubleshooting guide for common errors: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_s3_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_s3_uri\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\estimator.py:1376\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     forward_to_mlflow_tracking_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m-> 1376\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward_to_mlflow_tracking_server:\n\u001b[0;32m   1378\u001b[0m     log_sagemaker_job_to_mlflow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\estimator.py:2750\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m   2748\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[0;32m   2749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2750\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\session.py:5945\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[1;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[0;32m   5924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   5925\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[0;32m   5926\u001b[0m \n\u001b[0;32m   5927\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5943\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[0;32m   5944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5945\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\session.py:8547\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[1;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[0;32m   8544\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[0;32m   8546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m-> 8547\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[0;32m   8549\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\session.py:8611\u001b[0m, in \u001b[0;36m_check_job_status\u001b[1;34m(job, desc, status_key_name)\u001b[0m\n\u001b[0;32m   8605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[0;32m   8606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[0;32m   8607\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m   8608\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   8609\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m   8610\u001b[0m     )\n\u001b[1;32m-> 8611\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[0;32m   8612\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m   8613\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   8614\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m   8615\u001b[0m )\n",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2024-10-13-15-15-50-943: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.8 train.py --do_eval True --do_train True --eval_steps 500 --evaluation_strategy steps --fp16 True --logging_steps 100 --model_name_or_path tiiuae/falcon-7b-instruct --num_train_epochs 1 --output_dir s3://falcon-artifact/ --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --save_steps 1000 --train_file s3://fine-tuning-training-data/train_data.json --validation_file s3://fine-tuning-training-data/test_data.json\"\n\rDownloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 26.0/26.0 [00:00<00:00, 41.9kB/s]\n\rDownloading:   0%|          | 0.00/665 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 665/665 [00:00<00:00, 995kB/s]\n/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transform, exit code: 1. Check troubleshooting guide for common errors: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({\"train\": train_data_s3_uri, \"test\": test_data_s3_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Sagemaker Again\n",
    "\n",
    "Following the tutorial here: https://github.com/huggingface/notebooks/blob/main/sagemaker/28_train_llms_with_qlora/sagemaker-notebook.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.232.2-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.35.49-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs<24,>=23.1.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathos\n",
      "  Downloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker\n",
      "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (24.1)\n",
      "Requirement already satisfied: pandas in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: requests in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: pyyaml~=6.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (6.0.2)\n",
      "Collecting sagemaker-core<2.0.0,>=1.0.0\n",
      "  Downloading sagemaker_core-1.0.10-py3-none-any.whl (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.4/388.4 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata<7.0,>=1.4.0\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (1.26.3)\n",
      "Requirement already satisfied: jsonschema in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (4.23.0)\n",
      "Collecting schema\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tqdm in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (4.66.5)\n",
      "Requirement already satisfied: platformdirs in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (4.3.6)\n",
      "Collecting tblib<4,>=1.7.0\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting sagemaker-mlflow\n",
      "  Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting protobuf<5.0,>=3.12\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting smdebug-rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: psutil in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (6.1.0)\n",
      "Collecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3.0.0,>=1.26.8 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from sagemaker) (2.2.3)\n",
      "Collecting botocore<1.36.0,>=1.35.49\n",
      "  Downloading botocore-1.35.49-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.49->boto3) (2.9.0.post0)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Collecting rich<14.0.0,>=13.0.0\n",
      "  Downloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3.0.0,>=1.7.0\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting mock<5.0,>4.0\n",
      "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from jsonschema->sagemaker) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.35.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from requests->sagemaker) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from requests->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: six in ./falcon-instruct/falcon/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from pandas->sagemaker) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from pandas->sagemaker) (2024.2)\n",
      "Collecting dill>=0.3.9\n",
      "  Using cached dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Collecting pox>=0.3.5\n",
      "  Downloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
      "Collecting multiprocess>=0.70.17\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ppft>=1.7.6.9\n",
      "  Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mlflow>=2.8\n",
      "  Downloading mlflow-2.17.1-py3-none-any.whl (26.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Flask<4\n",
      "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting graphene<4\n",
      "  Downloading graphene-3.4-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow<18,>=4.0.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (17.0.0)\n",
      "Collecting sqlalchemy<3,>=1.4.0\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting mlflow-skinny==2.17.1\n",
      "  Downloading mlflow_skinny-2.17.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting alembic!=1.10.0,<2\n",
      "  Using cached alembic-1.13.3-py3-none-any.whl (233 kB)\n",
      "Collecting matplotlib<4\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy<2\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.3)\n",
      "Collecting gunicorn<24\n",
      "  Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Collecting scikit-learn<2\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown<4,>=3.3\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitpython<4,>=3.1.9\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting databricks-sdk<1,>=0.20.0\n",
      "  Downloading databricks_sdk-0.36.0-py3-none-any.whl (569 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6,>=5.0.0\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0\n",
      "  Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Collecting click<9,>=7.0\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0\n",
      "  Using cached opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Using cached sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.9.0)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.18.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Werkzeug>=3.0.0\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting blinker>=1.6.2\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting graphql-core<3.3,>=3.1\n",
      "  Downloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting graphql-relay<3.3,>=3.1\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.1.5)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (10.2.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "Collecting google-auth~=2.0\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1<0.7.0,>=0.4.6\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: schema, zipp, wrapt, Werkzeug, threadpoolctl, tblib, sqlparse, smmap, smdebug-rulesconfig, scipy, pyparsing, pydantic-core, pyasn1, protobuf, ppft, pox, mock, mdurl, markdown, Mako, kiwisolver, joblib, jmespath, itsdangerous, gunicorn, greenlet, graphql-core, google-pasta, fonttools, dill, cycler, contourpy, cloudpickle, click, cachetools, blinker, attrs, annotated-types, sqlalchemy, scikit-learn, rsa, pydantic, pyasn1-modules, multiprocess, matplotlib, markdown-it-py, importlib-metadata, graphql-relay, gitdb, Flask, docker, deprecated, botocore, s3transfer, rich, pathos, opentelemetry-api, graphene, google-auth, gitpython, alembic, opentelemetry-semantic-conventions, databricks-sdk, boto3, sagemaker-core, opentelemetry-sdk, mlflow-skinny, mlflow, sagemaker-mlflow, sagemaker\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 24.2.0\n",
      "    Uninstalling attrs-24.2.0:\n",
      "      Successfully uninstalled attrs-24.2.0\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.0.2 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 3.0.2 requires multiprocess<0.70.17, but you have multiprocess 0.70.17 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.0.3 Mako-1.3.6 Werkzeug-3.0.6 alembic-1.13.3 annotated-types-0.7.0 attrs-23.2.0 blinker-1.8.2 boto3-1.35.49 botocore-1.35.49 cachetools-5.5.0 click-8.1.7 cloudpickle-2.2.1 contourpy-1.3.0 cycler-0.12.1 databricks-sdk-0.36.0 deprecated-1.2.14 dill-0.3.9 docker-7.1.0 fonttools-4.54.1 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.35.0 google-pasta-0.2.0 graphene-3.4 graphql-core-3.2.5 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 importlib-metadata-6.11.0 itsdangerous-2.2.0 jmespath-1.0.1 joblib-1.4.2 kiwisolver-1.4.7 markdown-3.7 markdown-it-py-3.0.0 matplotlib-3.9.2 mdurl-0.1.2 mlflow-2.17.1 mlflow-skinny-2.17.1 mock-4.0.3 multiprocess-0.70.17 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 pathos-0.3.3 pox-0.3.5 ppft-1.7.6.9 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.9.2 pydantic-core-2.23.4 pyparsing-3.2.0 rich-13.9.3 rsa-4.9 s3transfer-0.10.3 sagemaker-2.232.2 sagemaker-core-1.0.10 sagemaker-mlflow-0.1.0 schema-0.7.7 scikit-learn-1.5.2 scipy-1.14.1 smdebug-rulesconfig-1.0.1 smmap-5.0.1 sqlalchemy-2.0.36 sqlparse-0.5.1 tblib-3.0.0 threadpoolctl-3.5.0 wrapt-1.16.0 zipp-3.20.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::590184030535:role/service-role/AmazonSageMaker-ExecutionRole-20240420T185478\n",
      "sagemaker bucket: sagemaker-us-east-1-590184030535\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = \"/mnt/c/Users/RaviB/GitHub/vegan-ai-nutritionist/.env\"\n",
    "\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "sagemaker_role = os.getenv(\"SAGEMAKER_ROLE\")\n",
    "\n",
    "\n",
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    #aws_session_token=aws_session_token  # Optional\n",
    ")\n",
    "\n",
    "sess = sagemaker.Session(boto_session=boto3_session)\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {sagemaker_role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a47f14fe383425db3790231dcdc3666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054f16f469e94776addd5577f5c1bb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'What are 3 things to consider when picking a mountain to ski at?', 'context': '', 'response': 'Here are 3 things you should consider when picking a mountain visit: \\n* Skill level - some mountains have runs that are better suited toward beginners or experts.\\n* Conditions - some mountains might have gotten better or worse snow throughout the season. Weather can vary widely as well on any given day, so be sure to check ahead of time. \\n* Budget - the prices of ski lift tickets and ski gear can vary widely, check to see if each mountain fits your criteria to have a good day on the slopes.', 'category': 'general_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "From the passage provided, extract the movie that did well across the globe but received underwhelming traction in Matt Damon's home country.\n",
      "\n",
      "### Context\n",
      "In 2014, he starred in George Clooney's The Monuments Men, and played the minor role of scientist Dr. Mann, in Christopher Nolan's Interstellar. That same year, Damon appeared as a celebrity correspondent for Years of Living Dangerously In 2015, Damon portrayed the main character, astronaut Mark Watney, in Ridley Scott's The Martian, based on Andy Weir's best-selling novel of the same name, a role that earned him the Golden Globe Award for Best Actor – Motion Picture Musical or Comedy and his second Academy Award nomination for Best Actor. Having not returned for the fourth film in the Bourne film series, Damon reprised his role in 2016's Jason Bourne, reuniting with Paul Greengrass. In 2017, Damon played the lead role in Zhang Yimou's The Great Wall, a hit internationally and a disappointment at the domestic box office. The film, and Damon's casting, were not well received by critics. Later in 2017, he starred in two satires, George Clooney's 1950s-set Suburbicon, which was released in October, and Alexander Payne's comedy Downsizing, which was released in December In 2019, Damon portrayed Carroll Shelby in the action biographical drama Ford v Ferrari, directed by James Mangold.\n",
      "\n",
      "### Answer\n",
      "Zhang Yimou's The Great Wall performed well globally but did not meet domestic box office expectations.\n"
     ]
    }
   ],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970ce679297f4535a219fd4f2a9ebcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What is a Pythagorean triple?\n",
      "\n",
      "### Answer\n",
      "In mathematics, a Pythagorean triple consists of three positive integers, a, b, c, such that a² + b² = c². These integers can form the sides of a right triangle, with c as the hypotenuse. For example, (3, 4, 5) is a Pythagorean triple because 3² + 4² = 5².<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faa104c29ef47f2821c654667a36541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2636 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7862353155cd4c8f8e3e9a6ba06943c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1368\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-2024.10.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from s3fs) (3.10.10)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4\n",
      "  Downloading aiobotocore-2.15.2-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec==2024.10.0.*\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.35.37,>=1.35.16\n",
      "  Downloading botocore-1.35.36-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aioitertools<1.0.0,>=0.5.1\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./falcon-instruct/falcon/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Installing collected packages: fsspec, aioitertools, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.49\n",
      "    Uninstalling botocore-1.35.49:\n",
      "      Successfully uninstalled botocore-1.35.49\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.0.2 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\n",
      "boto3 1.35.49 requires botocore<1.36.0,>=1.35.49, but you have botocore 1.35.36 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.15.2 aioitertools-0.12.0 botocore-1.35.36 fsspec-2024.10.0 s3fs-2024.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24129714efcc4ebca9a46aea1c9a45f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-590184030535/processed/dolly/train\n"
     ]
    }
   ],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/processed/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "  'epochs': 1,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 4,                    # batch size for training\n",
    "  'lr': 2e-4,                                          # learning rate used during training\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge', # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = sagemaker_role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',            # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    dependencies         = ['scripts/requirements.txt'], # dependencies to install with training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'training': training_input_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-26 02:28:00 Starting - Starting the training job\n",
      "2024-10-26 02:28:00 Pending - Training job waiting for capacity......\n",
      "2024-10-26 02:28:37 Pending - Preparing the instances for training...\n",
      "2024-10-26 02:29:30 Downloading - Downloading the training image....................................\n",
      "2024-10-26 02:35:29 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-10-26 02:35:46,814 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-10-26 02:35:46,832 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-26 02:35:46,843 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-10-26 02:35:46,845 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-10-26 02:35:48,645 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/transformers.git@2ab75add4b30c2fc44a8bf575156d448d9ed87a7 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git (to revision 2ab75add4b30c2fc44a8bf575156d448d9ed87a7) to /tmp/pip-req-build-ee9jkwfo\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-ee9jkwfo\u001b[0m\n",
      "\u001b[34mRunning command git rev-parse -q --verify 'sha^2ab75add4b30c2fc44a8bf575156d448d9ed87a7'\u001b[0m\n",
      "\u001b[34mRunning command git fetch -q https://github.com/huggingface/transformers.git 2ab75add4b30c2fc44a8bf575156d448d9ed87a7\u001b[0m\n",
      "\u001b[34mRunning command git checkout -q 2ab75add4b30c2fc44a8bf575156d448d9ed87a7\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit 2ab75add4b30c2fc44a8bf575156d448d9ed87a7\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.40.2 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 3)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 3)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 3)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 3)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0.dev0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0.dev0->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0.dev0->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0.dev0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0.dev0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0->-r requirements.txt (line 1)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 4.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 435.0/435.0 kB 46.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.32.0.dev0-py3-none-any.whl size=7383845 sha256=615d90f7435eb2fb1c19a19201b0dc24acc454254a6851c67ef94c896c4419d7\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ca/f7/0b/28ff67b882c57abe1fbdec791d3710a56c8363a52a77c20396\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, transformers, accelerate, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.4.5 transformers-4.32.0.dev0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-10-26 02:36:27,479 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-10-26 02:36:27,479 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-10-26 02:36:27,520 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-26 02:36:27,550 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-26 02:36:27,579 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-26 02:36:27,591 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 1,\n",
      "        \"lr\": 0.0002,\n",
      "        \"model_id\": \"tiiuae/falcon-7b\",\n",
      "        \"per_device_train_batch_size\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-590184030535/huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"lr\":0.0002,\"model_id\":\"tiiuae/falcon-7b\",\"per_device_train_batch_size\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-590184030535/huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"lr\":0.0002,\"model_id\":\"tiiuae/falcon-7b\",\"per_device_train_batch_size\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-590184030535/huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"1\",\"--lr\",\"0.0002\",\"--model_id\",\"tiiuae/falcon-7b\",\"--per_device_train_batch_size\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=tiiuae/falcon-7b\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 1 --lr 0.0002 --model_id tiiuae/falcon-7b --per_device_train_batch_size 4\u001b[0m\n",
      "\u001b[34m2024-10-26 02:36:27,630 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 7.82MB/s]\u001b[0m\n",
      "\u001b[34mconfiguration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfiguration_falcon.py: 100%|██████████| 7.16k/7.16k [00:00<00:00, 38.6MB/s]\u001b[0m\n",
      "\u001b[34mA new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\u001b[0m\n",
      "\u001b[34m- configuration_falcon.py\u001b[0m\n",
      "\u001b[34m. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\u001b[0m\n",
      "\u001b[34mA new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\u001b[0m\n",
      "\u001b[34m- configuration_falcon.py\u001b[0m\n",
      "\u001b[34m. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\u001b[0m\n",
      "\u001b[34mWARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\u001b[0m\n",
      "\u001b[34mmodeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mmodeling_falcon.py: 100%|██████████| 56.9k/56.9k [00:00<00:00, 59.8MB/s]\u001b[0m\n",
      "\u001b[34mA new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\u001b[0m\n",
      "\u001b[34m- modeling_falcon.py\u001b[0m\n",
      "\u001b[34m. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\u001b[0m\n",
      "\u001b[34mA new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\u001b[0m\n",
      "\u001b[34m- modeling_falcon.py\u001b[0m\n",
      "\u001b[34m. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json: 100%|██████████| 17.7k/17.7k [00:00<00:00, 85.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 21.0M/9.95G [00:00<00:53, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 41.9M/9.95G [00:00<01:06, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|          | 73.4M/9.95G [00:00<00:52, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|          | 105M/9.95G [00:00<00:46, 211MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|▏         | 136M/9.95G [00:00<00:50, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 168M/9.95G [00:00<00:46, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 199M/9.95G [00:00<00:46, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 231M/9.95G [00:01<00:43, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 262M/9.95G [00:01<00:42, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 294M/9.95G [00:01<00:43, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 325M/9.95G [00:01<00:43, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▎         | 357M/9.95G [00:01<00:41, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▍         | 388M/9.95G [00:01<00:40, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▍         | 419M/9.95G [00:01<00:40, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▍         | 451M/9.95G [00:02<00:40, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▍         | 482M/9.95G [00:02<00:40, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▌         | 514M/9.95G [00:02<00:40, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▌         | 545M/9.95G [00:02<00:39, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▌         | 577M/9.95G [00:02<00:38, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▌         | 608M/9.95G [00:02<00:46, 200MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▋         | 640M/9.95G [00:02<00:43, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 671M/9.95G [00:03<00:42, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 703M/9.95G [00:03<00:41, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 734M/9.95G [00:03<00:42, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 765M/9.95G [00:03<00:40, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 797M/9.95G [00:03<00:38, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 828M/9.95G [00:03<00:39, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▊         | 860M/9.95G [00:03<00:39, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 891M/9.95G [00:04<00:40, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 923M/9.95G [00:04<00:41, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|▉         | 954M/9.95G [00:04<00:44, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|▉         | 986M/9.95G [00:04<00:41, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|█         | 1.02G/9.95G [00:04<00:41, 213MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.05G/9.95G [00:04<00:40, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.08G/9.95G [00:04<00:39, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.11G/9.95G [00:05<00:37, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█▏        | 1.14G/9.95G [00:05<00:36, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.17G/9.95G [00:05<00:36, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.21G/9.95G [00:05<00:38, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.24G/9.95G [00:05<00:38, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.27G/9.95G [00:05<00:37, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.30G/9.95G [00:05<00:36, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.33G/9.95G [00:05<00:36, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▎        | 1.36G/9.95G [00:06<00:39, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▍        | 1.39G/9.95G [00:06<00:37, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▍        | 1.43G/9.95G [00:06<00:37, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▍        | 1.46G/9.95G [00:06<00:36, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▍        | 1.49G/9.95G [00:06<00:36, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▌        | 1.52G/9.95G [00:06<00:35, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.55G/9.95G [00:06<00:38, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.58G/9.95G [00:07<00:37, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.61G/9.95G [00:07<00:36, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.65G/9.95G [00:07<00:35, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.68G/9.95G [00:07<00:38, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.71G/9.95G [00:07<00:37, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.74G/9.95G [00:07<00:35, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.77G/9.95G [00:07<00:35, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.80G/9.95G [00:08<00:36, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.84G/9.95G [00:08<00:42, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.87G/9.95G [00:08<00:39, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.90G/9.95G [00:08<00:38, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.93G/9.95G [00:08<00:37, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|█▉        | 1.96G/9.95G [00:08<00:35, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|██        | 1.99G/9.95G [00:08<00:35, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|██        | 2.02G/9.95G [00:09<00:34, 229MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██        | 2.06G/9.95G [00:09<00:35, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██        | 2.09G/9.95G [00:09<00:37, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██▏       | 2.12G/9.95G [00:09<00:37, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.15G/9.95G [00:09<00:35, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.18G/9.95G [00:09<00:37, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.21G/9.95G [00:09<00:35, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.24G/9.95G [00:10<00:38, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.26G/9.95G [00:10<00:47, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.29G/9.95G [00:10<00:48, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.31G/9.95G [00:10<00:52, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.33G/9.95G [00:11<01:05, 116MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▎       | 2.35G/9.95G [00:11<01:17, 97.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.37G/9.95G [00:11<01:17, 97.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.39G/9.95G [00:11<01:24, 89.6MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.41G/9.95G [00:12<01:21, 92.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.42G/9.95G [00:12<01:24, 88.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.43G/9.95G [00:12<01:31, 82.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.45G/9.95G [00:12<01:18, 95.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.46G/9.95G [00:12<01:22, 90.4MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.47G/9.95G [00:12<01:21, 92.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.49G/9.95G [00:12<01:23, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▌       | 2.51G/9.95G [00:13<01:12, 103MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▌       | 2.52G/9.95G [00:13<01:20, 92.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▌       | 2.53G/9.95G [00:13<01:26, 85.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.54G/9.95G [00:13<01:29, 82.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.56G/9.95G [00:13<01:12, 102MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.57G/9.95G [00:13<01:15, 98.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.58G/9.95G [00:13<01:22, 89.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.60G/9.95G [00:13<01:05, 112MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▋       | 2.62G/9.95G [00:14<01:17, 95.0MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▋       | 2.63G/9.95G [00:14<01:19, 92.1MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.64G/9.95G [00:14<01:22, 88.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.65G/9.95G [00:14<01:24, 86.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.67G/9.95G [00:14<01:22, 88.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.69G/9.95G [00:15<01:11, 102MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.71G/9.95G [00:15<01:15, 96.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.72G/9.95G [00:15<01:18, 92.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.73G/9.95G [00:15<01:27, 82.2MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.75G/9.95G [00:15<01:09, 104MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.77G/9.95G [00:15<00:56, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.80G/9.95G [00:15<00:43, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.83G/9.95G [00:15<00:37, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.86G/9.95G [00:16<00:34, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.89G/9.95G [00:16<00:33, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.93G/9.95G [00:16<00:37, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|██▉       | 2.96G/9.95G [00:16<00:34, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|███       | 2.99G/9.95G [00:16<00:33, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|███       | 3.02G/9.95G [00:16<00:31, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.05G/9.95G [00:16<00:30, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.08G/9.95G [00:17<00:30, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███▏      | 3.11G/9.95G [00:17<00:30, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.15G/9.95G [00:17<00:29, 229MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.18G/9.95G [00:17<00:29, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.21G/9.95G [00:17<00:28, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.24G/9.95G [00:17<00:28, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.27G/9.95G [00:17<00:29, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.30G/9.95G [00:18<00:31, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▎      | 3.33G/9.95G [00:18<00:30, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.37G/9.95G [00:18<00:29, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.40G/9.95G [00:18<00:30, 213MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.43G/9.95G [00:18<00:30, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▍      | 3.46G/9.95G [00:18<00:30, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▌      | 3.49G/9.95G [00:18<00:28, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▌      | 3.52G/9.95G [00:19<00:28, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▌      | 3.55G/9.95G [00:19<00:29, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▌      | 3.59G/9.95G [00:19<00:29, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▋      | 3.62G/9.95G [00:19<00:30, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.65G/9.95G [00:19<00:29, 213MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.68G/9.95G [00:19<00:29, 213MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.71G/9.95G [00:19<00:27, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.74G/9.95G [00:20<00:26, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.77G/9.95G [00:20<00:27, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.81G/9.95G [00:20<00:26, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▊      | 3.84G/9.95G [00:20<00:26, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.87G/9.95G [00:20<00:25, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.90G/9.95G [00:20<00:25, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.93G/9.95G [00:20<00:25, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.96G/9.95G [00:20<00:25, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|████      | 4.00G/9.95G [00:21<00:24, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|████      | 4.03G/9.95G [00:21<00:24, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.06G/9.95G [00:21<00:25, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.09G/9.95G [00:21<00:25, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████▏     | 4.12G/9.95G [00:21<00:24, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.15G/9.95G [00:21<00:24, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.18G/9.95G [00:21<00:24, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.22G/9.95G [00:22<00:24, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.25G/9.95G [00:22<00:24, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.28G/9.95G [00:22<00:24, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.31G/9.95G [00:22<00:24, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▎     | 4.34G/9.95G [00:22<00:23, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.37G/9.95G [00:22<00:23, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.40G/9.95G [00:22<00:23, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▍     | 4.44G/9.95G [00:23<00:27, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▍     | 4.47G/9.95G [00:23<00:31, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▌     | 4.50G/9.95G [00:23<00:28, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▌     | 4.52G/9.95G [00:23<00:37, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.55G/9.95G [00:23<00:33, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.58G/9.95G [00:23<00:30, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▋     | 4.61G/9.95G [00:24<00:27, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.65G/9.95G [00:24<00:25, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.68G/9.95G [00:24<00:24, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.71G/9.95G [00:24<00:23, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.74G/9.95G [00:24<00:22, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.77G/9.95G [00:24<00:21, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.80G/9.95G [00:24<00:22, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▊     | 4.83G/9.95G [00:25<00:21, 234MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.87G/9.95G [00:25<00:21, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.90G/9.95G [00:25<00:21, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|████▉     | 4.93G/9.95G [00:25<00:21, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|████▉     | 4.96G/9.95G [00:25<00:23, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 4.99G/9.95G [00:26<00:39, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 5.01G/9.95G [00:26<00:46, 106MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.03G/9.95G [00:26<00:45, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.05G/9.95G [00:26<00:40, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.09G/9.95G [00:26<00:34, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████▏    | 5.11G/9.95G [00:27<00:34, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.13G/9.95G [00:27<00:53, 90.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.16G/9.95G [00:27<00:42, 113MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.18G/9.95G [00:27<00:44, 106MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.20G/9.95G [00:27<00:38, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.23G/9.95G [00:28<00:31, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.26G/9.95G [00:28<00:34, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.28G/9.95G [00:28<00:35, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.31G/9.95G [00:28<00:34, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▎    | 5.33G/9.95G [00:29<00:45, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▎    | 5.35G/9.95G [00:29<00:42, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.37G/9.95G [00:29<00:42, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.39G/9.95G [00:29<00:44, 103MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.41G/9.95G [00:29<00:45, 101MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▍    | 5.44G/9.95G [00:30<00:41, 110MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.47G/9.95G [00:30<00:37, 118MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.51G/9.95G [00:30<00:31, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.53G/9.95G [00:30<00:35, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.56G/9.95G [00:30<00:29, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.58G/9.95G [00:30<00:29, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▋    | 5.61G/9.95G [00:31<00:25, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.63G/9.95G [00:31<00:24, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.65G/9.95G [00:31<00:25, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.68G/9.95G [00:31<00:22, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.71G/9.95G [00:31<00:20, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.75G/9.95G [00:31<00:20, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.78G/9.95G [00:31<00:19, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.81G/9.95G [00:32<00:19, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▊    | 5.84G/9.95G [00:32<00:19, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.87G/9.95G [00:32<00:18, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.90G/9.95G [00:32<00:20, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|█████▉    | 5.93G/9.95G [00:32<00:19, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|█████▉    | 5.97G/9.95G [00:32<00:22, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|██████    | 5.99G/9.95G [00:32<00:21, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|██████    | 6.02G/9.95G [00:33<00:19, 201MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.05G/9.95G [00:33<00:23, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.08G/9.95G [00:33<00:20, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████▏   | 6.10G/9.95G [00:33<00:21, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.12G/9.95G [00:33<00:20, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.16G/9.95G [00:33<00:20, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.18G/9.95G [00:34<00:22, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.21G/9.95G [00:34<00:20, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.24G/9.95G [00:34<00:18, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.27G/9.95G [00:34<00:17, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.30G/9.95G [00:34<00:16, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▎   | 6.33G/9.95G [00:34<00:15, 229MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.36G/9.95G [00:34<00:15, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.40G/9.95G [00:34<00:15, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.43G/9.95G [00:35<00:15, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.46G/9.95G [00:35<00:15, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▌   | 6.49G/9.95G [00:35<00:15, 220MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.52G/9.95G [00:35<00:15, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.55G/9.95G [00:35<00:14, 229MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.59G/9.95G [00:35<00:14, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▋   | 6.62G/9.95G [00:35<00:14, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.65G/9.95G [00:36<00:15, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.68G/9.95G [00:36<00:14, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.71G/9.95G [00:36<00:14, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.74G/9.95G [00:36<00:14, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.77G/9.95G [00:36<00:13, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.81G/9.95G [00:36<00:13, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▊   | 6.84G/9.95G [00:36<00:13, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.87G/9.95G [00:37<00:12, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.90G/9.95G [00:37<00:12, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.93G/9.95G [00:37<00:12, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.96G/9.95G [00:37<00:13, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 6.99G/9.95G [00:37<00:12, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.03G/9.95G [00:37<00:12, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.06G/9.95G [00:37<00:12, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.09G/9.95G [00:37<00:11, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.12G/9.95G [00:38<00:11, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.15G/9.95G [00:38<00:11, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.18G/9.95G [00:38<00:11, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.21G/9.95G [00:38<00:11, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.25G/9.95G [00:38<00:11, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.28G/9.95G [00:38<00:11, 226MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.31G/9.95G [00:38<00:11, 238MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.34G/9.95G [00:39<00:11, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.37G/9.95G [00:39<00:10, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.40G/9.95G [00:39<00:11, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▍  | 7.43G/9.95G [00:39<00:13, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.47G/9.95G [00:39<00:12, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.50G/9.95G [00:39<00:12, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.53G/9.95G [00:39<00:11, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.56G/9.95G [00:40<00:11, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▋  | 7.59G/9.95G [00:40<00:11, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.62G/9.95G [00:40<00:11, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.65G/9.95G [00:40<00:10, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.69G/9.95G [00:40<00:10, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.72G/9.95G [00:40<00:09, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.75G/9.95G [00:41<00:13, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.77G/9.95G [00:41<00:20, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.79G/9.95G [00:41<00:18, 114MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▊  | 7.82G/9.95G [00:41<00:16, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.84G/9.95G [00:42<00:20, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.87G/9.95G [00:42<00:16, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.91G/9.95G [00:42<00:13, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|███████▉  | 7.93G/9.95G [00:42<00:12, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|███████▉  | 7.96G/9.95G [00:42<00:11, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 7.99G/9.95G [00:43<00:13, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.02G/9.95G [00:43<00:11, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.05G/9.95G [00:43<00:12, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.07G/9.95G [00:43<00:12, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████▏ | 8.10G/9.95G [00:43<00:12, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.13G/9.95G [00:43<00:10, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.16G/9.95G [00:43<00:09, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.19G/9.95G [00:44<00:08, 196MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.22G/9.95G [00:44<00:09, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.25G/9.95G [00:44<00:08, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.27G/9.95G [00:44<00:11, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.30G/9.95G [00:44<00:09, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▎ | 8.33G/9.95G [00:44<00:09, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.36G/9.95G [00:45<00:08, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.38G/9.95G [00:45<00:08, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.40G/9.95G [00:45<00:07, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.43G/9.95G [00:45<00:07, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.46G/9.95G [00:45<00:06, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.49G/9.95G [00:45<00:06, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.52G/9.95G [00:45<00:06, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.56G/9.95G [00:45<00:06, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▋ | 8.59G/9.95G [00:46<00:06, 204MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.61G/9.95G [00:46<00:06, 200MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.63G/9.95G [00:46<00:07, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.65G/9.95G [00:46<00:07, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.68G/9.95G [00:46<00:06, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.71G/9.95G [00:46<00:05, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.75G/9.95G [00:46<00:05, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.78G/9.95G [00:47<00:05, 208MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▊ | 8.81G/9.95G [00:47<00:05, 221MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.84G/9.95G [00:47<00:05, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.87G/9.95G [00:47<00:04, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.90G/9.95G [00:47<00:04, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.93G/9.95G [00:47<00:04, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 8.97G/9.95G [00:47<00:04, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 9.00G/9.95G [00:48<00:03, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.03G/9.95G [00:48<00:03, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.06G/9.95G [00:48<00:03, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████▏| 9.09G/9.95G [00:48<00:03, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.12G/9.95G [00:48<00:03, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.15G/9.95G [00:48<00:03, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.19G/9.95G [00:48<00:03, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.22G/9.95G [00:48<00:03, 220MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.25G/9.95G [00:49<00:03, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.28G/9.95G [00:49<00:03, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▎| 9.31G/9.95G [00:49<00:02, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.34G/9.95G [00:49<00:02, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.37G/9.95G [00:49<00:02, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.41G/9.95G [00:49<00:02, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.44G/9.95G [00:49<00:02, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.47G/9.95G [00:50<00:02, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.50G/9.95G [00:50<00:01, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.53G/9.95G [00:50<00:01, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.56G/9.95G [00:50<00:01, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▋| 9.59G/9.95G [00:50<00:01, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.63G/9.95G [00:50<00:01, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.66G/9.95G [00:50<00:01, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.69G/9.95G [00:50<00:01, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.72G/9.95G [00:51<00:00, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.75G/9.95G [00:51<00:00, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.78G/9.95G [00:51<00:00, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▊| 9.81G/9.95G [00:51<00:00, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.85G/9.95G [00:51<00:00, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.88G/9.95G [00:51<00:00, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.91G/9.95G [00:51<00:00, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.94G/9.95G [00:52<00:00, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|██████████| 9.95G/9.95G [00:52<00:00, 191MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:52<00:52, 52.16s/it]\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   1%|          | 31.5M/4.48G [00:00<00:19, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   1%|▏         | 62.9M/4.48G [00:00<00:18, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   2%|▏         | 94.4M/4.48G [00:00<00:18, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   3%|▎         | 126M/4.48G [00:00<00:18, 240MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   4%|▎         | 157M/4.48G [00:00<00:18, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   4%|▍         | 189M/4.48G [00:00<00:20, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   5%|▍         | 220M/4.48G [00:00<00:20, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   6%|▌         | 252M/4.48G [00:01<00:19, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   6%|▋         | 283M/4.48G [00:01<00:18, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 315M/4.48G [00:01<00:17, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   8%|▊         | 346M/4.48G [00:01<00:17, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   8%|▊         | 377M/4.48G [00:01<00:17, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   9%|▉         | 409M/4.48G [00:01<00:17, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  10%|▉         | 440M/4.48G [00:01<00:16, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  11%|█         | 472M/4.48G [00:02<00:16, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  11%|█         | 503M/4.48G [00:02<00:16, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  12%|█▏        | 535M/4.48G [00:02<00:16, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  13%|█▎        | 566M/4.48G [00:02<00:16, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  13%|█▎        | 598M/4.48G [00:02<00:16, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  14%|█▍        | 629M/4.48G [00:02<00:15, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  15%|█▍        | 661M/4.48G [00:02<00:15, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  15%|█▌        | 692M/4.48G [00:02<00:15, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  16%|█▌        | 724M/4.48G [00:03<00:15, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  17%|█▋        | 755M/4.48G [00:03<00:16, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  18%|█▊        | 786M/4.48G [00:03<00:15, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  18%|█▊        | 818M/4.48G [00:03<00:15, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  19%|█▉        | 849M/4.48G [00:03<00:14, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  20%|█▉        | 881M/4.48G [00:03<00:14, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  20%|██        | 912M/4.48G [00:03<00:14, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  21%|██        | 944M/4.48G [00:03<00:14, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  22%|██▏       | 975M/4.48G [00:04<00:14, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  22%|██▏       | 1.01G/4.48G [00:04<00:14, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  23%|██▎       | 1.04G/4.48G [00:04<00:14, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  24%|██▍       | 1.07G/4.48G [00:04<00:14, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▍       | 1.10G/4.48G [00:04<00:14, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▌       | 1.13G/4.48G [00:04<00:14, 237MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  26%|██▌       | 1.16G/4.48G [00:04<00:13, 238MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  27%|██▋       | 1.20G/4.48G [00:05<00:13, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  27%|██▋       | 1.23G/4.48G [00:05<00:13, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  28%|██▊       | 1.26G/4.48G [00:05<00:14, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  29%|██▉       | 1.29G/4.48G [00:05<00:14, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  29%|██▉       | 1.32G/4.48G [00:05<00:13, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  30%|███       | 1.35G/4.48G [00:05<00:13, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  31%|███       | 1.38G/4.48G [00:05<00:14, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  32%|███▏      | 1.42G/4.48G [00:06<00:14, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  32%|███▏      | 1.45G/4.48G [00:06<00:13, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  33%|███▎      | 1.48G/4.48G [00:06<00:15, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▎      | 1.51G/4.48G [00:06<00:14, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▍      | 1.54G/4.48G [00:06<00:13, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  35%|███▌      | 1.57G/4.48G [00:06<00:12, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▌      | 1.60G/4.48G [00:06<00:12, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▋      | 1.64G/4.48G [00:07<00:11, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.67G/4.48G [00:07<00:11, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  38%|███▊      | 1.70G/4.48G [00:07<00:15, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▊      | 1.73G/4.48G [00:07<00:14, 189MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▉      | 1.76G/4.48G [00:07<00:13, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  40%|███▉      | 1.79G/4.48G [00:07<00:12, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  41%|████      | 1.82G/4.48G [00:08<00:12, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  41%|████▏     | 1.86G/4.48G [00:08<00:11, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  42%|████▏     | 1.89G/4.48G [00:08<00:13, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.92G/4.48G [00:08<00:12, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  44%|████▎     | 1.95G/4.48G [00:08<00:11, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  44%|████▍     | 1.98G/4.48G [00:08<00:11, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  45%|████▍     | 2.01G/4.48G [00:08<00:10, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▌     | 2.04G/4.48G [00:09<00:10, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▋     | 2.08G/4.48G [00:09<00:10, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  47%|████▋     | 2.11G/4.48G [00:09<00:13, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  47%|████▋     | 2.13G/4.48G [00:09<00:17, 138MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  48%|████▊     | 2.16G/4.48G [00:09<00:14, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  49%|████▉     | 2.19G/4.48G [00:09<00:12, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  50%|████▉     | 2.22G/4.48G [00:10<00:11, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  50%|█████     | 2.25G/4.48G [00:10<00:11, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  51%|█████     | 2.29G/4.48G [00:10<00:10, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 2.32G/4.48G [00:10<00:11, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 2.34G/4.48G [00:10<00:12, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 2.36G/4.48G [00:10<00:13, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 2.38G/4.48G [00:11<00:16, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  54%|█████▎    | 2.40G/4.48G [00:11<00:15, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  54%|█████▍    | 2.42G/4.48G [00:11<00:15, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  54%|█████▍    | 2.44G/4.48G [00:11<00:15, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▍    | 2.46G/4.48G [00:11<00:14, 142MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▌    | 2.50G/4.48G [00:11<00:13, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▌    | 2.52G/4.48G [00:12<00:13, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  57%|█████▋    | 2.54G/4.48G [00:12<00:12, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  57%|█████▋    | 2.56G/4.48G [00:12<00:15, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.58G/4.48G [00:12<00:15, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.60G/4.48G [00:12<00:14, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.62G/4.48G [00:12<00:14, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▉    | 2.64G/4.48G [00:13<00:15, 117MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▉    | 2.66G/4.48G [00:13<00:15, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  60%|█████▉    | 2.68G/4.48G [00:13<00:16, 109MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  60%|██████    | 2.71G/4.48G [00:13<00:14, 125MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████    | 2.73G/4.48G [00:13<00:16, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████▏   | 2.75G/4.48G [00:14<00:18, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.76G/4.48G [00:14<00:18, 91.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.78G/4.48G [00:14<00:18, 94.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.80G/4.48G [00:14<00:17, 97.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  63%|██████▎   | 2.82G/4.48G [00:14<00:14, 113MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▎   | 2.85G/4.48G [00:14<00:11, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▍   | 2.88G/4.48G [00:15<00:09, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  65%|██████▌   | 2.92G/4.48G [00:15<00:08, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  66%|██████▌   | 2.95G/4.48G [00:15<00:07, 201MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  66%|██████▋   | 2.98G/4.48G [00:15<00:07, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 3.01G/4.48G [00:15<00:06, 218MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 3.04G/4.48G [00:15<00:06, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▊   | 3.07G/4.48G [00:15<00:06, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▉   | 3.10G/4.48G [00:16<00:05, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  70%|██████▉   | 3.14G/4.48G [00:16<00:05, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████   | 3.17G/4.48G [00:16<00:05, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████▏  | 3.20G/4.48G [00:16<00:05, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  72%|███████▏  | 3.23G/4.48G [00:16<00:05, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  73%|███████▎  | 3.26G/4.48G [00:16<00:05, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  73%|███████▎  | 3.29G/4.48G [00:16<00:04, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▍  | 3.32G/4.48G [00:16<00:04, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▍  | 3.36G/4.48G [00:17<00:04, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  76%|███████▌  | 3.39G/4.48G [00:17<00:04, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  76%|███████▌  | 3.42G/4.48G [00:17<00:04, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 3.45G/4.48G [00:17<00:04, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 3.48G/4.48G [00:17<00:04, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 3.51G/4.48G [00:17<00:04, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  79%|███████▉  | 3.54G/4.48G [00:17<00:04, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|███████▉  | 3.58G/4.48G [00:18<00:03, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|████████  | 3.61G/4.48G [00:18<00:03, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████  | 3.64G/4.48G [00:18<00:03, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  82%|████████▏ | 3.67G/4.48G [00:18<00:03, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  83%|████████▎ | 3.70G/4.48G [00:18<00:03, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  83%|████████▎ | 3.73G/4.48G [00:18<00:03, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▍ | 3.76G/4.48G [00:18<00:02, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  85%|████████▍ | 3.80G/4.48G [00:18<00:02, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  85%|████████▌ | 3.83G/4.48G [00:19<00:02, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▌ | 3.86G/4.48G [00:19<00:02, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.89G/4.48G [00:19<00:02, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.92G/4.48G [00:19<00:02, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  88%|████████▊ | 3.95G/4.48G [00:19<00:02, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▉ | 3.98G/4.48G [00:19<00:02, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|████████▉ | 4.02G/4.48G [00:19<00:01, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|█████████ | 4.05G/4.48G [00:19<00:01, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  91%|█████████ | 4.08G/4.48G [00:20<00:01, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  92%|█████████▏| 4.11G/4.48G [00:20<00:01, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  92%|█████████▏| 4.14G/4.48G [00:20<00:01, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 4.17G/4.48G [00:20<00:01, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  94%|█████████▍| 4.20G/4.48G [00:20<00:01, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  94%|█████████▍| 4.24G/4.48G [00:20<00:01, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  95%|█████████▌| 4.27G/4.48G [00:20<00:00, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  96%|█████████▌| 4.30G/4.48G [00:21<00:00, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  97%|█████████▋| 4.33G/4.48G [00:21<00:00, 229MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  97%|█████████▋| 4.36G/4.48G [00:21<00:00, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  98%|█████████▊| 4.39G/4.48G [00:21<00:00, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  99%|█████████▊| 4.42G/4.48G [00:21<00:00, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  99%|█████████▉| 4.46G/4.48G [00:21<00:00, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|██████████| 4.48G/4.48G [00:21<00:00, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|██████████| 4.48G/4.48G [00:21<00:00, 205MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [01:14<00:00, 34.35s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [01:14<00:00, 37.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.34s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|██████████| 117/117 [00:00<00:00, 928kB/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 130,547,712 || all params: 3,739,292,544 || trainable%: 3.4912409356543783\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/342 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/run_clm.py\", line 184, in <module>\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"/opt/ml/code/run_clm.py\", line 180, in main\u001b[0m\n",
      "\u001b[34mtraining_function(args)\n",
      "  File \"/opt/ml/code/run_clm.py\", line 146, in training_function\u001b[0m\n",
      "\u001b[34mtrainer.train()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1526, in train\u001b[0m\n",
      "\u001b[34mreturn inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1796, in _inner_training_loop\u001b[0m\n",
      "\u001b[34mtr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2641, in training_step\u001b[0m\n",
      "\u001b[34mloss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2666, in compute_loss\u001b[0m\n",
      "\u001b[34moutputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 922, in forward\u001b[0m\n",
      "\u001b[34mreturn self.base_model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34moutput = old_forward(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py\", line 900, in forward\u001b[0m\n",
      "\u001b[34mtransformer_outputs = self.transformer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34moutput = old_forward(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py\", line 789, in forward\u001b[0m\n",
      "\u001b[34moutputs = torch.utils.checkpoint.checkpoint(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\u001b[0m\n",
      "\u001b[34mreturn CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 107, in forward\u001b[0m\n",
      "\u001b[34moutputs = run_function(*args)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py\", line 785, in custom_forward\u001b[0m\n",
      "\u001b[34mreturn module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34moutput = old_forward(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py\", line 453, in forward\u001b[0m\n",
      "\u001b[34mattn_outputs = self.self_attention(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\u001b[0m\n",
      "\u001b[34moutput = old_forward(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/ec89142b67d748a1865ea4451372db8313ada0d8/modeling_falcon.py\", line 341, in forward\u001b[0m\n",
      "\u001b[34mattn_output = F.scaled_dot_product_attention(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.44 GiB (GPU 0; 15.77 GiB total capacity; 10.30 GiB already allocated; 797.44 MiB free; 14.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m0%|          | 0/342 [00:01<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2024-10-26 02:39:01,769 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-10-26 02:39:01,769 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-10-26 02:39:01,770 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2024-10-26 02:39:01,770 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.44 GiB (GPU 0; 15.77 GiB total capacity; 10.30 GiB already allocated; 797.44 MiB free; 14.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " 0%|          | 0/342 [00:01<?, ?it/s]\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 1 --lr 0.0002 --model_id tiiuae/falcon-7b --per_device_train_batch_size 4\"\u001b[0m\n",
      "\u001b[34m2024-10-26 02:39:01,770 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2024-10-26 02:39:18 Uploading - Uploading generated training model\n",
      "2024-10-26 02:39:18 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.44 GiB (GPU 0; 15.77 GiB total capacity; 10.30 GiB already allocated; 797.44 MiB free; 14.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n 0%|          | 0/342 [00:01<?, ?it/s]\"\nCommand \"/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 1 --lr 0.0002 --model_id tiiuae/falcon-7b --per_device_train_batch_size 4\", exit code: 1. Check troubleshooting guide for common errors: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhuggingface_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/falcon-instruct/falcon/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/falcon-instruct/falcon/lib/python3.10/site-packages/sagemaker/estimator.py:1376\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     forward_to_mlflow_tracking_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward_to_mlflow_tracking_server:\n\u001b[1;32m   1378\u001b[0m     log_sagemaker_job_to_mlflow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/mnt/d/falcon-instruct/falcon/lib/python3.10/site-packages/sagemaker/estimator.py:2750\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2750\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/mnt/d/falcon-instruct/falcon/lib/python3.10/site-packages/sagemaker/session.py:5945\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5925\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5926\u001b[0m \n\u001b[1;32m   5927\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5943\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5945\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/falcon-instruct/falcon/lib/python3.10/site-packages/sagemaker/session.py:8547\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   8544\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   8546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 8547\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   8549\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/mnt/d/falcon-instruct/falcon/lib/python3.10/site-packages/sagemaker/session.py:8611\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8607\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8608\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8609\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8610\u001b[0m     )\n\u001b[0;32m-> 8611\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8612\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8613\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8614\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8615\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-qlora-2024-10-25-23-27-51-2024-10-26-02-27-59-734: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.44 GiB (GPU 0; 15.77 GiB total capacity; 10.30 GiB already allocated; 797.44 MiB free; 14.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n 0%|          | 0/342 [00:01<?, ?it/s]\"\nCommand \"/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 1 --lr 0.0002 --model_id tiiuae/falcon-7b --per_device_train_batch_size 4\", exit code: 1. Check troubleshooting guide for common errors: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import LoraConfig, PeftConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/falcon-instruct'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/falcon-instruct/falcon/lib/python3.10/site-packages/nvidia/cuda_runtime/lib/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nvidia\n",
    "\n",
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.5.0+cu121\n",
      "bitsandbytes: 0.44.1\n",
      "transformers: 4.45.2\n",
      "peft: 0.13.2\n",
      "accelerate: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "\n",
    "# List of packages to check\n",
    "packages = [\"torch\", \"bitsandbytes\", \"transformers\", \"peft\", \"accelerate\"]\n",
    "\n",
    "# Loop through the packages and print their versions\n",
    "for package in packages:\n",
    "    try:\n",
    "        version = importlib.metadata.version(package)\n",
    "        print(f\"{package}: {version}\")\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        print(f\"{package}: Not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "\tload_in_4bit=True,\n",
    "\tbnb_4bit_use_double_quant=True,\n",
    "\tbnb_4bit_quant_type=\"nf4\",\n",
    "\tbnb_4bit_compute_dtype=torch.bfloat16a\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\"\n",
    "TRANSFORMERS_CACHE = \"D:/huggingface_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = \"/mnt/c/Users/RaviB/GitHub/vegan-ai-nutritionist/.env\"\n",
    "\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "HUGGINGFACE_ACCESS_TOKEN = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\")\n",
    "assert HUGGINGFACE_ACCESS_TOKEN is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HUGGINGFACE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8e46c210e44798a568c79e4d35752a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0387747fe7b74a1fbb912e8d6b5bb5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297c5fecd69f4551a60545755856a291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=TRANSFORMERS_CACHE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "\tr=4,\n",
    "\tlora_alpha=32,\n",
    "\ttarget_modules=[\n",
    "\t\t\"query_key_value\",\n",
    "\t\t\"dense\",\n",
    "\t\t\"dense_h_to_4h\",\n",
    "\t\t\"dense_4h_to_h\",\n",
    "\t],\n",
    "\tlora_dropout=0.05,\n",
    "\tbias=\"none\",\n",
    "\ttask_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8159232 || all params: 3616904064 || trainable%: 0.22558607736409123\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e109e9493c149ed9cf69b55cbf55c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.7z:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bee237b63f04824b420d0577a53a89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c6f698bdf947b19c4929e35893180b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14be3f32ba04e68a0459ca695aaa037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49974140d7d5403fa5c8413dbdf7a5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a804a5fc3acc4e1693ca95ff61604c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3fa15f71534d8793f600dcad37805a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"dialogue\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\ttrain_dataset=train_dataset,\n",
    "\teval_dataset=test_dataset,\n",
    "\targs=TrainingArguments(\n",
    "\t\tper_device_train_batch_size=8,\n",
    "\t\tper_device_eval_batch_size=8,\n",
    "\t\t#logging_dir=log_bucket,\n",
    "\t\tlogging_steps=2,\n",
    "\t\tnum_train_epochs=1,\n",
    "\t\tlearning_rate=2e-4,\n",
    "\t\tbf16=True,\n",
    "\t\tsave_strategy = \"no\",\n",
    "\t\toutput_dir=\"outputs\",\n",
    "\t\t#report_to=\"tensorboard\",\n",
    "\t),\n",
    "\tdata_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
