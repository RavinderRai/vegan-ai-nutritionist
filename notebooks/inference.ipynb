{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Here wil will expand and finish inference testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RaviB\\\\GitHub\\\\vegan-ai-nutritionist\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RaviB\\\\GitHub\\\\vegan-ai-nutritionist'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading ENV Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "sagemaker_role = os.getenv(\"SAGEMAKER_ROLE\")\n",
    "huggingface_access_token = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "sess = sagemaker.Session(boto_session=boto3_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.3.3-gpu-py310-cu121-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.3.3\"\n",
    ")\n",
    "\n",
    "print(llm_image)\n",
    "\n",
    "max_input_length = 1024\n",
    "max_total_tokens = 2048\n",
    "number_of_gpu = 1\n",
    "\n",
    "# Configuration for the model\n",
    "config = {\n",
    "    'HF_MODEL_ID': \"/opt/ml/model\",  # Path to where SageMaker stores the model\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),  # Number of GPUs used per replica (modify as needed)\n",
    "    'MAX_INPUT_LENGTH': json.dumps(max_input_length),  # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(max_total_tokens),  # Max length of the generation (including input text)\n",
    "    'trust_remote_code': 'True',\n",
    "    #'HF_MODEL_QUANTIZE': 'bitsandbytes'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring MLflow\n",
    "\n",
    "We need the model uri. We will want the latest one though, so we can go through the mlflow database and get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_id                                            8d92bc21177343dfa2dd5c17a9b22321\n",
       "experiment_id                                                                    0\n",
       "status                                                                    FINISHED\n",
       "artifact_uri                     file:///C:/Users/RaviB/GitHub/vegan-ai-nutriti...\n",
       "start_time                                        2024-11-01 15:25:28.078000+00:00\n",
       "end_time                                          2024-11-01 15:44:41.803000+00:00\n",
       "params.job_name                                   falcon-qlora-2024-11-01-12-25-26\n",
       "params.learning_rate                                                        0.0002\n",
       "params.epochs                                                                    1\n",
       "params.model_data_uri            s3://sagemaker-us-east-1-590184030535/falcon-q...\n",
       "params.batch_size                                                                2\n",
       "params.model_id                                          tiiuae/falcon-7b-instruct\n",
       "tags.mlflow.user                                                             RaviB\n",
       "tags.mlflow.source.name          C:\\Users\\RaviB\\GitHub\\vegan-ai-nutritionist\\mo...\n",
       "tags.mlflow.source.type                                                      LOCAL\n",
       "tags.mlflow.source.git.commit             e24b4ee69fa277c2197c4478680b8b06f3ad6346\n",
       "tags.mlflow.runName                                              skillful-mule-826\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow/mlflow.db\")\n",
    "\n",
    "runs = mlflow.search_runs(order_by=[\"start_time DESC\"])\n",
    "\n",
    "latest_run = runs.iloc[0]\n",
    "latest_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8d92bc21177343dfa2dd5c17a9b22321'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_run_id = latest_run.run_id\n",
    "latest_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_data_uri': 's3://sagemaker-us-east-1-590184030535/falcon-qlora-2024-11-01-12-25-26-2024-11-01-15-25-28-836/output/model.tar.gz',\n",
       " 'model_id': 'tiiuae/falcon-7b-instruct',\n",
       " 'job_name': 'falcon-qlora-2024-11-01-12-25-26',\n",
       " 'epochs': '1',\n",
       " 'batch_size': '2',\n",
       " 'learning_rate': '0.0002'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_run_params = mlflow.get_run(latest_run_id).data.params\n",
    "latest_run_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-590184030535/falcon-qlora-2024-11-01-12-25-26-2024-11-01-15-25-28-836/output/model.tar.gz'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_run_params['model_data_uri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function to do this all in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data_uri(params_to_retrieve=None, run_number=0, mlflow_data_path=\"sqlite:///mlflow/mlflow.db\"):\n",
    "    mlflow.set_tracking_uri(mlflow_data_path)\n",
    "\n",
    "    # Get the list of runs, sorted by start time in descending order\n",
    "    runs = mlflow.search_runs(order_by=[\"start_time DESC\"])\n",
    "\n",
    "    # Check if there are any runs\n",
    "    if runs.empty:\n",
    "        raise ValueError(\"No runs found in the specified MLflow database.\")\n",
    "\n",
    "    # Select the specified run\n",
    "    selected_run_id = runs.iloc[run_number].run_id\n",
    "\n",
    "    # Get the parameters of the selected run\n",
    "    selected_run_params = mlflow.get_run(selected_run_id).data.params\n",
    "\n",
    "    # If no specific parameters are provided, default to model_data_uri\n",
    "    if params_to_retrieve is None:\n",
    "        params_to_retrieve = ['model_data_uri']\n",
    "\n",
    "    # Retrieve the requested parameters\n",
    "    retrieved_params = {param: selected_run_params.get(param) for param in params_to_retrieve}\n",
    "\n",
    "    return retrieved_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_uri = get_model_data_uri()['model_data_uri']\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=sagemaker_role,\n",
    "    image_uri=llm_image,\n",
    "    model_data=s3_model_uri,\n",
    "    env=config,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------*"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please check the troubleshooting guide for common errors: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html#sagemaker-python-sdk-troubleshooting-create-endpoint\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint huggingface-pytorch-tgi-inference-2024-11-01-23-12-21-438: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mml.g5.2xlarge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Choose an instance type that suits your model and budget\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase timeout for loading large models\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\huggingface\\model.py:319\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[1;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     inference_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneuron\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.inf1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneuronx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserving_image_uri(\n\u001b[0;32m    314\u001b[0m         region_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mboto_session\u001b[38;5;241m.\u001b[39mregion_name,\n\u001b[0;32m    315\u001b[0m         instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[0;32m    316\u001b[0m         inference_tool\u001b[38;5;241m=\u001b[39minference_tool,\n\u001b[0;32m    317\u001b[0m     )\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(HuggingFaceModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[0;32m    320\u001b[0m     initial_instance_count,\n\u001b[0;32m    321\u001b[0m     instance_type,\n\u001b[0;32m    322\u001b[0m     serializer,\n\u001b[0;32m    323\u001b[0m     deserializer,\n\u001b[0;32m    324\u001b[0m     accelerator_type,\n\u001b[0;32m    325\u001b[0m     endpoint_name,\n\u001b[0;32m    326\u001b[0m     format_tags(tags),\n\u001b[0;32m    327\u001b[0m     kms_key,\n\u001b[0;32m    328\u001b[0m     wait,\n\u001b[0;32m    329\u001b[0m     data_capture_config,\n\u001b[0;32m    330\u001b[0m     async_inference_config,\n\u001b[0;32m    331\u001b[0m     serverless_inference_config,\n\u001b[0;32m    332\u001b[0m     volume_size\u001b[38;5;241m=\u001b[39mvolume_size,\n\u001b[0;32m    333\u001b[0m     model_data_download_timeout\u001b[38;5;241m=\u001b[39mmodel_data_download_timeout,\n\u001b[0;32m    334\u001b[0m     container_startup_health_check_timeout\u001b[38;5;241m=\u001b[39mcontainer_startup_health_check_timeout,\n\u001b[0;32m    335\u001b[0m     inference_recommendation_id\u001b[38;5;241m=\u001b[39minference_recommendation_id,\n\u001b[0;32m    336\u001b[0m     explainer_config\u001b[38;5;241m=\u001b[39mexplainer_config,\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    338\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\model.py:1753\u001b[0m, in \u001b[0;36mModel.deploy\u001b[1;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, model_reference_arn, **kwargs)\u001b[0m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[0;32m   1751\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[1;32m-> 1753\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[0;32m   1766\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\session.py:5820\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[1;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[0;32m   5817\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[0;32m   5818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[1;32m-> 5820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\session.py:4665\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[1;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[0;32m   4657\u001b[0m troubleshooting \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4658\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.aws.amazon.com/sagemaker/latest/dg/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4659\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker-python-sdk-troubleshooting.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4660\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#sagemaker-python-sdk-troubleshooting-create-endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4661\u001b[0m )\n\u001b[0;32m   4662\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m   4663\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the troubleshooting guide for common errors: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, troubleshooting\n\u001b[0;32m   4664\u001b[0m )\n\u001b[1;32m-> 4665\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\session.py:4654\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[1;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[0;32m   4651\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_arn \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   4653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m-> 4654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n\u001b[0;32m   4656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\vegan\\lib\\site-packages\\sagemaker\\session.py:5463\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[1;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[0;32m   5457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[0;32m   5458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[0;32m   5459\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m   5460\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   5461\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m   5462\u001b[0m         )\n\u001b[1;32m-> 5463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[0;32m   5464\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m   5465\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   5466\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m   5467\u001b[0m     )\n\u001b[0;32m   5468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint huggingface-pytorch-tgi-inference-2024-11-01-23-12-21-438: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",  # Choose an instance type that suits your model and budget\n",
    "    container_startup_health_check_timeout=300  # Increase timeout for loading large models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Base Model\n",
    "\n",
    "And if it fails, then we will deploy the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.3.3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Falcon 7b\n",
    "hub = {'HF_MODEL_ID':'tiiuae/falcon-7b-instruct'}\n",
    "\n",
    "# Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role=sagemaker_role,\n",
    "   image_uri=llm_image,\n",
    "   sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type='ml.g5.2xlarge',\n",
    " \tcontainer_startup_health_check_timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Endpoint\n",
    "\n",
    "We need to delete the endpoint once we decide we no longer need it, so we must get the endpoint name from the mlflow folder first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we already have a function to get the data from mlflow sqlite databases\n",
    "from modules.inference.src.model_utils import get_model_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'huggingface-pytorch-tgi-inference-2024-11-02-18-25-29-952',\n",
       " 'endpoint_name': 'huggingface-pytorch-tgi-inference-2024-11-02-18-25-31-152'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_endpoint = get_model_data_uri(['model_name', 'endpoint_name'], run_number=0, mlflow_data_path=\"sqlite:///mlflow/endpoints.db\")\n",
    "latest_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-tgi-inference-2024-11-02-18-25-29-952'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_endpoint['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HuggingFacePredictor: {'endpoint_name': 'huggingface-pytorch-tgi-inference-2024-11-02-18-07-05-569', 'sagemaker_session': <sagemaker.session.Session object at 0x00000235EEC4BDC0>, 'serializer': <sagemaker.base_serializers.JSONSerializer object at 0x00000235ED21F430>, 'deserializer': <sagemaker.base_deserializers.JSONDeserializer object at 0x00000235ED21E590>}\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_endpoint['endpoint_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils.utils import AWSConnector\n",
    "\n",
    "aws_connector = AWSConnector()\n",
    "\n",
    "aws_connector.delete_sagemaker_model('huggingface-pytorch-tgi-inference-2024-11-02-18-07-04-383')\n",
    "aws_connector.delete_sagemaker_endpoint('huggingface-pytorch-tgi-inference-2024-11-02-18-07-05-569')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
